{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef859c0-b3fc-4cc0-8415-da37beb4a933",
   "metadata": {},
   "source": [
    "### Data is cleaned and embedded using MPNet model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b728fcf5-b64a-4feb-8c09-ef3997991131",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /opt/conda/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.9/site-packages (from imblearn) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.23.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.9.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: Sentence_Transformers in /opt/conda/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.9/site-packages (from Sentence_Transformers) (4.40.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from Sentence_Transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from Sentence_Transformers) (1.12.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from Sentence_Transformers) (1.23.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from Sentence_Transformers) (1.1.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from Sentence_Transformers) (1.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.9/site-packages (from Sentence_Transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from Sentence_Transformers) (10.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->Sentence_Transformers) (3.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->Sentence_Transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->Sentence_Transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->Sentence_Transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->Sentence_Transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->Sentence_Transformers) (4.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->Sentence_Transformers) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->Sentence_Transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.34.0->Sentence_Transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->Sentence_Transformers) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->Sentence_Transformers) (3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->Sentence_Transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->Sentence_Transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->Sentence_Transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->Sentence_Transformers) (2022.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install Sentence_Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b2b8849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords#, PlainTextCorpusReader\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime, date, timedelta\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, LlamaForSequenceClassification\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706acbe0-b149-4a9d-b108-84786e15dd37",
   "metadata": {},
   "source": [
    "## Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c641a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\mehdi.sadeghi\\\\OneDrive - Georgia Institute of Technology\\\\Gatech\\\\ISYE 6740\\\\Project\\\\archive'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3188d762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\mehdi.sadeghi\\OneDrive - Georgia Institute of Technology\\Gatech\\ISYE 6740\\Project\\Emotions-main\\Emotions-main\\02_Model\n"
     ]
    }
   ],
   "source": [
    "new_directory = 'C:\\\\Users\\\\mehdi.sadeghi\\\\OneDrive - Georgia Institute of Technology\\\\Gatech\\\\ISYE 6740\\\\Project\\\\Emotions-main\\\\Emotions-main\\\\02_Model'\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Verify the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8df4806-9636-47b6-95f7-68cecfb5d461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>...</th>\n",
       "      <th>c_761</th>\n",
       "      <th>c_762</th>\n",
       "      <th>c_763</th>\n",
       "      <th>c_764</th>\n",
       "      <th>c_765</th>\n",
       "      <th>c_766</th>\n",
       "      <th>c_767</th>\n",
       "      <th>text_WO_stopwords</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057199</td>\n",
       "      <td>-0.008160</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>-0.049585</td>\n",
       "      <td>0.032901</td>\n",
       "      <td>0.057294</td>\n",
       "      <td>-0.069347</td>\n",
       "      <td>-0.019316</td>\n",
       "      <td>0.037468</td>\n",
       "      <td>-0.033207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011801</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>-0.056683</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>-0.009209</td>\n",
       "      <td>-0.009716</td>\n",
       "      <td>-0.033261</td>\n",
       "      <td>feel really helpless heavy hearted</td>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.057446</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>-0.021202</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>-0.020960</td>\n",
       "      <td>0.035543</td>\n",
       "      <td>-0.050522</td>\n",
       "      <td>0.016254</td>\n",
       "      <td>0.016209</td>\n",
       "      <td>0.036987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014699</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>-0.025727</td>\n",
       "      <td>-0.002154</td>\n",
       "      <td>0.029399</td>\n",
       "      <td>0.034281</td>\n",
       "      <td>-0.062547</td>\n",
       "      <td>ive enjoy able slouch relax unwind frankly nee...</td>\n",
       "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018095</td>\n",
       "      <td>-0.022132</td>\n",
       "      <td>-0.009757</td>\n",
       "      <td>-0.014497</td>\n",
       "      <td>0.006242</td>\n",
       "      <td>0.049693</td>\n",
       "      <td>-0.061562</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>0.054573</td>\n",
       "      <td>-0.015577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026375</td>\n",
       "      <td>-0.009772</td>\n",
       "      <td>0.015014</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>-0.007213</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>-0.037378</td>\n",
       "      <td>give internship dmrg feel distraught</td>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005479</td>\n",
       "      <td>-0.035476</td>\n",
       "      <td>-0.021814</td>\n",
       "      <td>-0.022414</td>\n",
       "      <td>0.043661</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>-0.041316</td>\n",
       "      <td>-0.009272</td>\n",
       "      <td>-0.022895</td>\n",
       "      <td>-0.023498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024940</td>\n",
       "      <td>-0.000361</td>\n",
       "      <td>0.031484</td>\n",
       "      <td>-0.021171</td>\n",
       "      <td>0.040770</td>\n",
       "      <td>-0.025041</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>dont know feel lose</td>\n",
       "      <td>i dont know i feel so lost</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021173</td>\n",
       "      <td>0.006722</td>\n",
       "      <td>-0.028587</td>\n",
       "      <td>-0.033321</td>\n",
       "      <td>0.020113</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.040073</td>\n",
       "      <td>0.028007</td>\n",
       "      <td>-0.029061</td>\n",
       "      <td>0.014258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>-0.006862</td>\n",
       "      <td>-0.090544</td>\n",
       "      <td>-0.012799</td>\n",
       "      <td>0.030897</td>\n",
       "      <td>0.026404</td>\n",
       "      <td>-0.063194</td>\n",
       "      <td>kindergarten teacher thoroughly weary job take...</td>\n",
       "      <td>i am a kindergarten teacher and i am thoroughl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416804</th>\n",
       "      <td>0.020868</td>\n",
       "      <td>0.042881</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.014421</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>-0.007785</td>\n",
       "      <td>0.026107</td>\n",
       "      <td>0.026876</td>\n",
       "      <td>-0.014553</td>\n",
       "      <td>-0.017963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.046698</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>-0.010543</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.010221</td>\n",
       "      <td>-0.015694</td>\n",
       "      <td>feel like tell horny devil find site suit sort...</td>\n",
       "      <td>i feel like telling these horny devils to find...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416805</th>\n",
       "      <td>-0.002463</td>\n",
       "      <td>0.040540</td>\n",
       "      <td>-0.000846</td>\n",
       "      <td>-0.065937</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>0.049785</td>\n",
       "      <td>-0.106255</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>0.041116</td>\n",
       "      <td>-0.013932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025246</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>-0.012717</td>\n",
       "      <td>-0.025871</td>\n",
       "      <td>0.030419</td>\n",
       "      <td>-0.013384</td>\n",
       "      <td>-0.035171</td>\n",
       "      <td>begin realize feel agitate restless would thin...</td>\n",
       "      <td>i began to realize that when i was feeling agi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416806</th>\n",
       "      <td>0.039020</td>\n",
       "      <td>0.039674</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>-0.031511</td>\n",
       "      <td>-0.064205</td>\n",
       "      <td>-0.000712</td>\n",
       "      <td>-0.017174</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>-0.006511</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>-0.033828</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>-0.009031</td>\n",
       "      <td>feel curious previous early dawn time seek tro...</td>\n",
       "      <td>i feel very curious be why previous early dawn...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416807</th>\n",
       "      <td>-0.029445</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>-0.025720</td>\n",
       "      <td>-0.017802</td>\n",
       "      <td>0.080966</td>\n",
       "      <td>0.025269</td>\n",
       "      <td>-0.034207</td>\n",
       "      <td>-0.013629</td>\n",
       "      <td>-0.004662</td>\n",
       "      <td>-0.011820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066382</td>\n",
       "      <td>0.010981</td>\n",
       "      <td>-0.058766</td>\n",
       "      <td>-0.056127</td>\n",
       "      <td>-0.026503</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.038368</td>\n",
       "      <td>feel becuase tyranical nature government el sa...</td>\n",
       "      <td>i feel that becuase of the tyranical nature of...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416808</th>\n",
       "      <td>0.068557</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>-0.023559</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.052985</td>\n",
       "      <td>-0.023038</td>\n",
       "      <td>-0.042416</td>\n",
       "      <td>-0.032413</td>\n",
       "      <td>0.036469</td>\n",
       "      <td>0.049974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>0.024286</td>\n",
       "      <td>-0.004236</td>\n",
       "      <td>-0.015013</td>\n",
       "      <td>0.039648</td>\n",
       "      <td>0.060178</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>think spend time investigate surround things s...</td>\n",
       "      <td>i think that after i had spent some time inves...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416809 rows × 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             c_0       c_1       c_2       c_3       c_4       c_5       c_6  \\\n",
       "0       0.057199 -0.008160  0.001413 -0.049585  0.032901  0.057294 -0.069347   \n",
       "1      -0.057446  0.005956 -0.021202 -0.015104 -0.020960  0.035543 -0.050522   \n",
       "2       0.018095 -0.022132 -0.009757 -0.014497  0.006242  0.049693 -0.061562   \n",
       "3       0.005479 -0.035476 -0.021814 -0.022414  0.043661  0.003678 -0.041316   \n",
       "4       0.021173  0.006722 -0.028587 -0.033321  0.020113  0.015797  0.040073   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "416804  0.020868  0.042881  0.011425  0.014421  0.003465 -0.007785  0.026107   \n",
       "416805 -0.002463  0.040540 -0.000846 -0.065937  0.008002  0.049785 -0.106255   \n",
       "416806  0.039020  0.039674  0.004809 -0.031511 -0.064205 -0.000712 -0.017174   \n",
       "416807 -0.029445  0.039162 -0.025720 -0.017802  0.080966  0.025269 -0.034207   \n",
       "416808  0.068557  0.013036 -0.023559  0.011761  0.052985 -0.023038 -0.042416   \n",
       "\n",
       "             c_7       c_8       c_9  ...     c_761     c_762     c_763  \\\n",
       "0      -0.019316  0.037468 -0.033207  ... -0.011801 -0.000416 -0.056683   \n",
       "1       0.016254  0.016209  0.036987  ... -0.014699 -0.043307 -0.025727   \n",
       "2      -0.009043  0.054573 -0.015577  ...  0.026375 -0.009772  0.015014   \n",
       "3      -0.009272 -0.022895 -0.023498  ... -0.024940 -0.000361  0.031484   \n",
       "4       0.028007 -0.029061  0.014258  ...  0.017429 -0.006862 -0.090544   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "416804  0.026876 -0.014553 -0.017963  ...  0.014526  0.046698  0.001502   \n",
       "416805  0.024187  0.041116 -0.013932  ... -0.025246 -0.001675 -0.012717   \n",
       "416806  0.001122 -0.006511  0.003020  ...  0.001483 -0.002141  0.000917   \n",
       "416807 -0.013629 -0.004662 -0.011820  ...  0.066382  0.010981 -0.058766   \n",
       "416808 -0.032413  0.036469  0.049974  ...  0.006980  0.024286 -0.004236   \n",
       "\n",
       "           c_764     c_765     c_766     c_767  \\\n",
       "0      -0.003689 -0.009209 -0.009716 -0.033261   \n",
       "1      -0.002154  0.029399  0.034281 -0.062547   \n",
       "2       0.032342 -0.007213  0.007759 -0.037378   \n",
       "3      -0.021171  0.040770 -0.025041  0.021277   \n",
       "4      -0.012799  0.030897  0.026404 -0.063194   \n",
       "...          ...       ...       ...       ...   \n",
       "416804 -0.010543  0.000936  0.010221 -0.015694   \n",
       "416805 -0.025871  0.030419 -0.013384 -0.035171   \n",
       "416806 -0.033828  0.012037  0.009011 -0.009031   \n",
       "416807 -0.056127 -0.026503  0.008378  0.038368   \n",
       "416808 -0.015013  0.039648  0.060178  0.034511   \n",
       "\n",
       "                                        text_WO_stopwords  \\\n",
       "0                      feel really helpless heavy hearted   \n",
       "1       ive enjoy able slouch relax unwind frankly nee...   \n",
       "2                    give internship dmrg feel distraught   \n",
       "3                                     dont know feel lose   \n",
       "4       kindergarten teacher thoroughly weary job take...   \n",
       "...                                                   ...   \n",
       "416804  feel like tell horny devil find site suit sort...   \n",
       "416805  begin realize feel agitate restless would thin...   \n",
       "416806  feel curious previous early dawn time seek tro...   \n",
       "416807  feel becuase tyranical nature government el sa...   \n",
       "416808  think spend time investigate surround things s...   \n",
       "\n",
       "                                                     text  label  \n",
       "0           i just feel really helpless and heavy hearted      4  \n",
       "1       ive enjoyed being able to slouch about relax a...      0  \n",
       "2       i gave up my internship with the dmrg and am f...      4  \n",
       "3                              i dont know i feel so lost      0  \n",
       "4       i am a kindergarten teacher and i am thoroughl...      4  \n",
       "...                                                   ...    ...  \n",
       "416804  i feel like telling these horny devils to find...      2  \n",
       "416805  i began to realize that when i was feeling agi...      3  \n",
       "416806  i feel very curious be why previous early dawn...      5  \n",
       "416807  i feel that becuase of the tyranical nature of...      3  \n",
       "416808  i think that after i had spent some time inves...      5  \n",
       "\n",
       "[416809 rows x 771 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_data=pd.read_csv('./embeddings/mpnet_embed_df_part400000.csv') \n",
    "embd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89501b16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416809, 771)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618dbe25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = embd_data.drop('label', axis=1)\n",
    "y = embd_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90d1e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving Train and test data for later use in all modeling efforts\n",
    "train.to_csv('./train_df.csv')\n",
    "test.to_csv('./test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e745b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1- Using Embedded data from MPNet model and LLama Lite pre-trained model as classifier for emotion classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a468b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbeddedDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    def __getitem__(self, idx):\n",
    "        embedding=self.embeddings[idx]\n",
    "        embedding=torch.tensor(embedding, dtype=torch.float32)\n",
    "        embedding=embedding.unsqueeze(0)\n",
    "        return embedding, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a15925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "135ee28e",
   "metadata": {},
   "source": [
    "# No need to run this cell as our labels is already encoded\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(train['label'])\n",
    "train['label_coded'] = label_encoder.transform(train['label'])\n",
    "test['label_coded'] = label_encoder.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e107c367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  312606 \n",
      "test:  104203\n"
     ]
    }
   ],
   "source": [
    "print('train: ', len(train), '\\ntest: ', len(test)) #, '\\nval: ', len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd869a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['label'] = train['label'].astype('category')\n",
    "test['label'] = test['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a6f56d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c_0                   float64\n",
       "c_1                   float64\n",
       "c_2                   float64\n",
       "c_3                   float64\n",
       "c_4                   float64\n",
       "                       ...   \n",
       "c_766                 float64\n",
       "c_767                 float64\n",
       "text_WO_stopwords      object\n",
       "text                   object\n",
       "label                category\n",
       "Length: 771, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff49ba1",
   "metadata": {},
   "source": [
    "## Prepare model input for model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81e5fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_embeddings= np.stack(train.iloc[:,:768].values)\n",
    "train_labels= train['label'].values\n",
    "train_dataset= EmbeddedDataset(train_embeddings, train_labels)\n",
    "train_loader= DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cc74985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embeddings= np.stack(test.iloc[:,:768].values)\n",
    "test_labels= test['label'].values\n",
    "test_dataset= EmbeddedDataset(test_embeddings, test_labels)\n",
    "test_loader= DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a7ef960-e379-4030-add2-9a0f0b479489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_labels= len(train['label'].unique())\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6767abae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at skeskinen/llama-lite-134m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AdamW\n",
    "model_name='skeskinen/llama-lite-134m'\n",
    "model=LlamaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "optimizer= AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c0b681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.47116596780176606\n",
      "Test Loss: 0.6818288690473404, Accuracy: 0.750822901725769\n",
      "Epoch: 2, Loss: 0.25431708495673394\n",
      "Test Loss: 0.7496792136891488, Accuracy: 0.7564273476600647\n",
      "Epoch: 3, Loss: 0.1716895796681544\n",
      "Test Loss: 0.8472883035251699, Accuracy: 0.7544216513633728\n",
      "Epoch: 4, Loss: 0.13620078459216822\n",
      "Test Loss: 0.9292120997525432, Accuracy: 0.7511683702468872\n",
      "Epoch: 5, Loss: 0.12082830793036509\n",
      "Test Loss: 0.9896049231839327, Accuracy: 0.7497192621231079\n",
      "Early stopping Triggered\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "best_test_loss=float('inf')\n",
    "patience=3\n",
    "patience_counter=0\n",
    "best_accuracy=-1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss= 0\n",
    "    for batch in train_loader:\n",
    "        embeddings=batch[0].to(device)\n",
    "        labels=batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "        loss= outputs.loss\n",
    "        total_loss+= loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss= total_loss / len(train_loader)\n",
    "    print (f'Epoch: {epoch+1}, Loss: {average_loss}')\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss= 0\n",
    "    correct_predictions= 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            embeddings=batch[0].to(device)\n",
    "            labels=batch[1].to(device)\n",
    "            \n",
    "            outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "            test_loss+= outputs.loss.item()\n",
    "            \n",
    "            logits= outputs.logits\n",
    "            _, predicted_labels= torch.max(logits, dim=1)\n",
    "            correct_predictions+= torch.sum(predicted_labels==labels)\n",
    "    average_test_loss = test_loss/len(test_loader)\n",
    "    accuracy=correct_predictions/len(test_dataset)\n",
    "    print(f'Test Loss: {average_test_loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy= accuracy\n",
    "        patience_counter=0\n",
    "        torch.save({'model.state_dict()': model.state_dict(), 'optimizer.state_dict()': optimizer.state_dict()}, 'model_unbalanced.pt')\n",
    "        \n",
    "        torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, model_path) \n",
    "        \n",
    "        patience_counter=0\n",
    "    else:\n",
    "        patience_counter+=1\n",
    "    if patience_counter>=patience:\n",
    "        print('Early stopping Triggered')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60cf3e39-9716-417e-8ba1-e2012bff4322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36d8bc3b-fc43-4e50-8f85-d18bf2806735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at skeskinen/llama-lite-134m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using saved best model\n",
    "model_name='skeskinen/llama-lite-134m'\n",
    "model1=LlamaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "optimizer =AdamW(model1.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = torch.load('model_unbalanced.pt')\n",
    "optimizer.load_state_dict(checkpoint['optimizer.state_dict()'])\n",
    "model1.load_state_dict(checkpoint['model.state_dict()'])\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b400ebce-4a71-44c6-99b8-20c2252643ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the performance of model trained with unbalanced data on test dataset\n",
    "test_embeddings= np.stack(test.iloc[:,:768].values)\n",
    "test_labels= test['label'].values\n",
    "test_dataset= EmbeddedDataset(test_embeddings, test_labels)\n",
    "test_loader= DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de0f3bb8-ae0c-4c5b-9240-c690ab8d67cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device, data_df):\n",
    "    correct_predictions = 0\n",
    "    data_out_df_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            embeddings = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            outputs = model(inputs_embeds=embeddings)\n",
    "            logits = outputs.logits\n",
    "            _, predicted_labels = torch.max(logits, dim=1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            \n",
    "            df = pd.DataFrame(embeddings.cpu().numpy().reshape(embeddings.shape[0], embeddings.shape[2]))\n",
    "            df.columns = ['_' + str(x) for x in df.columns.values]\n",
    "            df['Actual'] = labels.cpu()\n",
    "            df['Pred'] = predicted_labels.cpu()\n",
    "            \n",
    "            data_out_df_list.append(df)\n",
    "        \n",
    "    accuracy = correct_predictions / len(data_loader.dataset)\n",
    "    print('Accuracy=', accuracy)\n",
    "        \n",
    "    data_df_res = pd.concat(data_out_df_list, ignore_index=True)\n",
    "        \n",
    "    print('Accuracy:', accuracy_score(data_df_res['Actual'], data_df_res['Pred']))\n",
    "    print('Balanced Accuracy:', balanced_accuracy_score(data_df_res['Actual'], data_df_res['Pred']))\n",
    "    print('F1 score:', f1_score(data_df_res['Actual'], data_df_res['Pred'], average='micro'))\n",
    "    print('Recall:', metrics.recall_score(data_df_res['Actual'], data_df_res['Pred'], average='micro'))\n",
    "    print('Precision:', metrics.precision_score(data_df_res['Actual'], data_df_res['Pred'], average='micro'))\n",
    "    \n",
    "    return data_df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e622f05-c48a-4066-94d4-38597215ba68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.7564273581374816\n",
      "Accuracy: 0.7564273581374816\n",
      "Balanced Accuracy: 0.7196425923862521\n",
      "F1 score: 0.7564273581374816\n",
      "Recall: 0.7564273581374816\n",
      "Precision: 0.7564273581374816\n"
     ]
    }
   ],
   "source": [
    "test_df_res = evaluate_model(model1, test_loader, device, test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e9e5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2- Balancing train dataset for potential model improvement and use same LLama pre-trained model for fine-tuning and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40d557f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     11229\n",
       "2     25915\n",
       "4     35784\n",
       "3     42988\n",
       "0     90890\n",
       "1    105800\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = train['label'].value_counts()\n",
    "# Sort the counts from smallest to largest\n",
    "class_counts_sorted = class_counts.sort_values()\n",
    "class_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e9216d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train=train.iloc[:,:768]\n",
    "y_train=train['label']\n",
    "sm=SMOTE(random_state=25, n_jobs=-1, k_neighbors=5)\n",
    "X_train_sm, y_train_sm= sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a20055aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(634800, 768)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60162a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    105800\n",
       "1    105800\n",
       "2    105800\n",
       "3    105800\n",
       "4    105800\n",
       "5    105800\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sm.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b16abb49-9fdd-4db6-aa68-128ae253ba2f",
   "metadata": {},
   "source": [
    "del train, X_train, y_train, class_counts, class_counts_sorted, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35fced0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_embeddings= np.stack(X_train_sm.iloc[:,:768].values)\n",
    "train_labels= y_train_sm.values\n",
    "train_dataset= EmbeddedDataset(train_embeddings, train_labels)\n",
    "train_loader= DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb906995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embeddings= np.stack(test.iloc[:,:768].values)\n",
    "test_labels= test['label'].values\n",
    "test_dataset= EmbeddedDataset(test_embeddings, test_labels)\n",
    "test_loader= DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adf2736c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_labels= len(train['label'].unique())\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac2f1e32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fed39c1ef3741b394882b13e427f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/535 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e0b670895d406bb188bfa05106751e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at skeskinen/llama-lite-134m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AdamW\n",
    "model_name='skeskinen/llama-lite-134m'\n",
    "model=LlamaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "optimizer= AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3187535-6de4-442b-93d1-115fb9990ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((312606, 771), (104203, 771))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape , test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "880f1921-b7c0-4afa-81b0-b51261fcb042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6910204766863348\n",
      "Test Loss: 0.6035168456884981, Accuracy: 0.767338752746582\n",
      "Epoch: 2, Loss: 0.4778885061117259\n",
      "Test Loss: 0.5801429836662269, Accuracy: 0.7747953534126282\n",
      "Epoch: 3, Loss: 0.31834184839205326\n",
      "Test Loss: 0.6402754418323376, Accuracy: 0.7651699185371399\n",
      "Epoch: 4, Loss: 0.20605498998503288\n",
      "Test Loss: 0.7783686681759138, Accuracy: 0.7620317935943604\n",
      "Epoch: 5, Loss: 0.15999099656915108\n",
      "Test Loss: 0.8979340281954572, Accuracy: 0.7577421069145203\n",
      "Early stopping Triggered\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "best_test_loss=float('inf')\n",
    "patience=3\n",
    "patience_counter=0\n",
    "best_accuracy=-1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss= 0\n",
    "    for batch in train_loader:\n",
    "        embeddings=batch[0].to(device)\n",
    "        labels=batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "        loss= outputs.loss\n",
    "        total_loss+= loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss= total_loss / len(train_loader)\n",
    "    print (f'Epoch: {epoch+1}, Loss: {average_loss}')\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss= 0\n",
    "    correct_predictions= 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            embeddings=batch[0].to(device)\n",
    "            labels=batch[1].to(device)\n",
    "            \n",
    "            outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "            test_loss+= outputs.loss.item()\n",
    "            \n",
    "            logits= outputs.logits\n",
    "            _, predicted_labels= torch.max(logits, dim=1)\n",
    "            correct_predictions+= torch.sum(predicted_labels==labels)\n",
    "    average_test_loss = test_loss/len(test_loader)\n",
    "    accuracy=correct_predictions/len(test_dataset)\n",
    "    print(f'Test Loss: {average_test_loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy= accuracy\n",
    "        patience_counter=0\n",
    "        torch.save({'model.state_dict()': model.state_dict(), 'optimizer.state_dict()': optimizer.state_dict()}, 'model_balanced.pt')\n",
    "        patience_counter=0\n",
    "    else:\n",
    "        patience_counter+=1\n",
    "    if patience_counter>=patience:\n",
    "        print('Early stopping Triggered')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dee180b4-2b31-4bb1-8c4d-e9becf41928d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at skeskinen/llama-lite-134m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using saved best model\n",
    "model_name='skeskinen/llama-lite-134m'\n",
    "model1=LlamaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "optimizer =AdamW(model1.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = torch.load('model_Balanced.pt')\n",
    "optimizer.load_state_dict(checkpoint['optimizer.state_dict()'])\n",
    "model1.load_state_dict(checkpoint['model.state_dict()'])\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac84952b-3052-4b10-8240-938e68ce7683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.7747953513814382\n",
      "Accuracy: 0.7747953513814382\n",
      "Balanced Accuracy: 0.7143492474497553\n",
      "F1 score: 0.7747953513814382\n",
      "Recall: 0.7747953513814382\n",
      "Precision: 0.7747953513814382\n"
     ]
    }
   ],
   "source": [
    "test_df_res = evaluate_model(model1, test_loader, device, test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b64cd9-5205-4a42-ae04-3534f688ec80",
   "metadata": {},
   "source": [
    "### Balancing data slightly improved the performance of the model, which cause increase in accuracy from 75.6% on epoch 2 to 77.5% again on epoch 2 when using balanced data."
   ]
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
