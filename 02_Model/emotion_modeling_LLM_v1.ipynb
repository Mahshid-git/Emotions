{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef859c0-b3fc-4cc0-8415-da37beb4a933",
   "metadata": {},
   "source": [
    "### Data is cleaned and embedded using MPNet model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b2b8849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords#, PlainTextCorpusReader\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, balanced_accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706acbe0-b149-4a9d-b108-84786e15dd37",
   "metadata": {},
   "source": [
    "## Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c641a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\mehdi.sadeghi\\\\OneDrive - Georgia Institute of Technology\\\\Gatech\\\\ISYE 6740\\\\Project\\\\archive'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3188d762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\mehdi.sadeghi\\OneDrive - Georgia Institute of Technology\\Gatech\\ISYE 6740\\Project\\Emotions-main\\Emotions-main\\02_Model\n"
     ]
    }
   ],
   "source": [
    "new_directory = 'C:\\\\Users\\\\mehdi.sadeghi\\\\OneDrive - Georgia Institute of Technology\\\\Gatech\\\\ISYE 6740\\\\Project\\\\Emotions-main\\\\Emotions-main\\\\02_Model'\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Verify the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f42975b5-158e-4649-bd4c-cace2d037168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5797c62f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416788</th>\n",
       "      <td>416804</td>\n",
       "      <td>416804</td>\n",
       "      <td>i feel like telling these horny devils to find...</td>\n",
       "      <td>2</td>\n",
       "      <td>feel like tell horny devil find site suit sort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416789</th>\n",
       "      <td>416805</td>\n",
       "      <td>416805</td>\n",
       "      <td>i began to realize that when i was feeling agi...</td>\n",
       "      <td>3</td>\n",
       "      <td>begin realize feel agitate restless think go dish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416790</th>\n",
       "      <td>416806</td>\n",
       "      <td>416806</td>\n",
       "      <td>i feel very curious be why previous early dawn...</td>\n",
       "      <td>5</td>\n",
       "      <td>feel curious previous early dawn time seek tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416791</th>\n",
       "      <td>416807</td>\n",
       "      <td>416807</td>\n",
       "      <td>i feel that becuase of the tyranical nature of...</td>\n",
       "      <td>3</td>\n",
       "      <td>feel because tyrannical nature government el s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416792</th>\n",
       "      <td>416808</td>\n",
       "      <td>416808</td>\n",
       "      <td>i think that after i had spent some time inves...</td>\n",
       "      <td>5</td>\n",
       "      <td>think spend time investigate surround things s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1  Unnamed: 0  \\\n",
       "416788        416804      416804   \n",
       "416789        416805      416805   \n",
       "416790        416806      416806   \n",
       "416791        416807      416807   \n",
       "416792        416808      416808   \n",
       "\n",
       "                                                     text  label  \\\n",
       "416788  i feel like telling these horny devils to find...      2   \n",
       "416789  i began to realize that when i was feeling agi...      3   \n",
       "416790  i feel very curious be why previous early dawn...      5   \n",
       "416791  i feel that becuase of the tyranical nature of...      3   \n",
       "416792  i think that after i had spent some time inves...      5   \n",
       "\n",
       "                                               clean_text  \n",
       "416788  feel like tell horny devil find site suit sort...  \n",
       "416789  begin realize feel agitate restless think go dish  \n",
       "416790  feel curious previous early dawn time seek tro...  \n",
       "416791  feel because tyrannical nature government el s...  \n",
       "416792  think spend time investigate surround things s...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embd_data=pd.read_csv('./embeddings/mpnet_embed_df_part400000.csv') \n",
    "#clean_data=pd.read_csv('./clean_text.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89501b16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "embd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618dbe25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = embd_data.drop('label', axis=1)\n",
    "y = embd_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90d1e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving Train and test data for later use in all modeling efforts\n",
    "train.to_csv('./train_df.csv')\n",
    "test.to_csv('./test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df117a87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>...</th>\n",
       "      <th>c_761</th>\n",
       "      <th>c_762</th>\n",
       "      <th>c_763</th>\n",
       "      <th>c_764</th>\n",
       "      <th>c_765</th>\n",
       "      <th>c_766</th>\n",
       "      <th>c_767</th>\n",
       "      <th>text_WO_stopwords</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342462</th>\n",
       "      <td>0.005049</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>-0.033797</td>\n",
       "      <td>-0.011138</td>\n",
       "      <td>-0.024820</td>\n",
       "      <td>0.018492</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>-0.017906</td>\n",
       "      <td>0.047792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.024708</td>\n",
       "      <td>-0.039280</td>\n",
       "      <td>-0.020336</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>-0.060829</td>\n",
       "      <td>-0.017347</td>\n",
       "      <td>feel im torture slowly today</td>\n",
       "      <td>i feel as if im being tortured very slowly today</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87990</th>\n",
       "      <td>-0.039426</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>-0.022700</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>-0.009668</td>\n",
       "      <td>-0.055998</td>\n",
       "      <td>0.037705</td>\n",
       "      <td>-0.007174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043236</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.007713</td>\n",
       "      <td>-0.046003</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>-0.012333</td>\n",
       "      <td>ill feel really bad</td>\n",
       "      <td>ill feel really bad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381594</th>\n",
       "      <td>0.027580</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>-0.028398</td>\n",
       "      <td>-0.019655</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>-0.028050</td>\n",
       "      <td>-0.067037</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>-0.047099</td>\n",
       "      <td>0.023724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>-0.061392</td>\n",
       "      <td>-0.028997</td>\n",
       "      <td>0.022805</td>\n",
       "      <td>0.026034</td>\n",
       "      <td>-0.006839</td>\n",
       "      <td>feel content whatever news tell know rise chal...</td>\n",
       "      <td>i feel content with whatever news we are told ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208217</th>\n",
       "      <td>-0.002705</td>\n",
       "      <td>0.047667</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.026343</td>\n",
       "      <td>0.015890</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>-0.091406</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.029813</td>\n",
       "      <td>0.026739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031983</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.035115</td>\n",
       "      <td>0.008799</td>\n",
       "      <td>0.015763</td>\n",
       "      <td>0.038264</td>\n",
       "      <td>-0.037314</td>\n",
       "      <td>love feel invigorate arrive work</td>\n",
       "      <td>i love feeling invigorated when i arrive at work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376976</th>\n",
       "      <td>0.014338</td>\n",
       "      <td>-0.047544</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>-0.060139</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.072453</td>\n",
       "      <td>0.019001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019103</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>-0.013725</td>\n",
       "      <td>-0.008387</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>feel acceptable use first many polish get rece...</td>\n",
       "      <td>i feel is acceptable i used the first of many ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             c_0       c_1       c_2       c_3       c_4       c_5       c_6  \\\n",
       "342462  0.005049 -0.004521 -0.033797 -0.011138 -0.024820  0.018492 -0.015220   \n",
       "87990  -0.039426 -0.004666 -0.022700  0.006759 -0.026521  0.036558 -0.009668   \n",
       "381594  0.027580  0.060547 -0.028398 -0.019655  0.017900 -0.028050 -0.067037   \n",
       "208217 -0.002705  0.047667 -0.001580 -0.026343  0.015890  0.024639 -0.091406   \n",
       "376976  0.014338 -0.047544  0.025093  0.011402 -0.006114 -0.007457 -0.060139   \n",
       "\n",
       "             c_7       c_8       c_9  ...     c_761     c_762     c_763  \\\n",
       "342462  0.012329 -0.017906  0.047792  ...  0.021197  0.024708 -0.039280   \n",
       "87990  -0.055998  0.037705 -0.007174  ... -0.043236  0.020760  0.006370   \n",
       "381594  0.006156 -0.047099  0.023724  ...  0.000482  0.009941 -0.061392   \n",
       "208217  0.014512  0.029813  0.026739  ...  0.031983 -0.009349 -0.035115   \n",
       "376976 -0.041030 -0.072453  0.019001  ... -0.019103  0.007613 -0.013725   \n",
       "\n",
       "           c_764     c_765     c_766     c_767  \\\n",
       "342462 -0.020336  0.022013 -0.060829 -0.017347   \n",
       "87990   0.007713 -0.046003 -0.000759 -0.012333   \n",
       "381594 -0.028997  0.022805  0.026034 -0.006839   \n",
       "208217  0.008799  0.015763  0.038264 -0.037314   \n",
       "376976 -0.008387  0.000549  0.007308  0.002215   \n",
       "\n",
       "                                        text_WO_stopwords  \\\n",
       "342462                       feel im torture slowly today   \n",
       "87990                                 ill feel really bad   \n",
       "381594  feel content whatever news tell know rise chal...   \n",
       "208217                   love feel invigorate arrive work   \n",
       "376976  feel acceptable use first many polish get rece...   \n",
       "\n",
       "                                                     text  label  \n",
       "342462   i feel as if im being tortured very slowly today      4  \n",
       "87990                                 ill feel really bad      0  \n",
       "381594  i feel content with whatever news we are told ...      1  \n",
       "208217   i love feeling invigorated when i arrive at work      1  \n",
       "376976  i feel is acceptable i used the first of many ...      1  \n",
       "\n",
       "[5 rows x 771 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973a58a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Requirement already satisfied: imbalanced-learn in /home/ubuntu/scm/lib/python3.8/site-packages (from imblearn) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/ubuntu/.local/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=0.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/ubuntu/scm/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ubuntu/scm/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Installing collected packages: imblearn\n",
      "Successfully installed imblearn-0.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/ubuntu/scm/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/scm/lib/python3.8/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Sentence_Transformers in /home/ubuntu/scm/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (4.30.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (from Sentence_Transformers) (1.22.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (0.22.1)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (1.8.1)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (2.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (23.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/scm/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->Sentence_Transformers) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/scm/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->Sentence_Transformers) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/ubuntu/scm/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->Sentence_Transformers) (3.26.4)\n",
      "Requirement already satisfied: lit in /home/ubuntu/scm/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->Sentence_Transformers) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->Sentence_Transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->Sentence_Transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->Sentence_Transformers) (0.3.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk->Sentence_Transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk->Sentence_Transformers) (1.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from torchvision->Sentence_Transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->Sentence_Transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->Sentence_Transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->Sentence_Transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->Sentence_Transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->Sentence_Transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/scm/lib/python3.8/site-packages (from sympy->torch>=1.6.0->Sentence_Transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install Sentence_Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e745b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1- Using Embedded data from MPNet model and LLama Lite pre-trained model as classifier for emotion classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a468b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbeddedDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    def __getitem__(self, idx):\n",
    "        embedding=self.embeddings[idx]\n",
    "        embedding=torch.tensor(embedding, dtype=torch.float32)\n",
    "        embedding=embedding.unsqueeze(0)\n",
    "        return embedding, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d7f285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /mnt/code/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a15925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "135ee28e",
   "metadata": {},
   "source": [
    "# No need to run this cell as our labels is already encoded\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(train['label'])\n",
    "train['label_coded'] = label_encoder.transform(train['label'])\n",
    "test['label_coded'] = label_encoder.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e107c367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  312606 \n",
      "test:  104203\n"
     ]
    }
   ],
   "source": [
    "print('train: ', len(train), '\\ntest: ', len(test)) #, '\\nval: ', len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd869a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['label'] = train['label'].astype('category')\n",
    "test['label'] = test['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a6f56d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c_0                   float64\n",
       "c_1                   float64\n",
       "c_2                   float64\n",
       "c_3                   float64\n",
       "c_4                   float64\n",
       "                       ...   \n",
       "c_766                 float64\n",
       "c_767                 float64\n",
       "text_WO_stopwords      object\n",
       "text                   object\n",
       "label                category\n",
       "Length: 771, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff49ba1",
   "metadata": {},
   "source": [
    "## Prepare model input for model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f81e5fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_embeddings= np.stack(train.iloc[:,:768].values)\n",
    "train_labels= train['label'].values\n",
    "train_dataset= EmbeddedDataset(train_embeddings, train_labels)\n",
    "train_loader= DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cc74985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embeddings= np.stack(test.iloc[:,:768].values)\n",
    "test_labels= test['label'].values\n",
    "test_dataset= EmbeddedDataset(test_embeddings, test_labels)\n",
    "test_loader= DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a7ef960-e379-4030-add2-9a0f0b479489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_labels= len(train['label'].unique())\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6767abae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at skeskinen/llama-lite-134m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AdamW\n",
    "model_name='skeskinen/llama-lite-134m'\n",
    "model=LlamaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "optimizer= AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c0b681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.47116596780176606\n",
      "Test Loss: 0.6818288690473404, Accuracy: 0.750822901725769\n",
      "Epoch: 2, Loss: 0.25431708495673394\n",
      "Test Loss: 0.7496792136891488, Accuracy: 0.7564273476600647\n",
      "Epoch: 3, Loss: 0.1716895796681544\n",
      "Test Loss: 0.8472883035251699, Accuracy: 0.7544216513633728\n",
      "Epoch: 4, Loss: 0.13620078459216822\n",
      "Test Loss: 0.9292120997525432, Accuracy: 0.7511683702468872\n",
      "Epoch: 5, Loss: 0.12082830793036509\n",
      "Test Loss: 0.9896049231839327, Accuracy: 0.7497192621231079\n",
      "Early stopping Triggered\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "best_test_loss=float('inf')\n",
    "patience=3\n",
    "patience_counter=0\n",
    "best_accuracy=-1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss= 0\n",
    "    for batch in train_loader:\n",
    "        embeddings=batch[0].to(device)\n",
    "        labels=batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "        loss= outputs.loss\n",
    "        total_loss+= loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss= total_loss / len(train_loader)\n",
    "    print (f'Epoch: {epoch+1}, Loss: {average_loss}')\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss= 0\n",
    "    correct_predictions= 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            embeddings=batch[0].to(device)\n",
    "            labels=batch[1].to(device)\n",
    "            \n",
    "            outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "            test_loss+= outputs.loss.item()\n",
    "            \n",
    "            logits= outputs.logits\n",
    "            _, predicted_labels= torch.max(logits, dim=1)\n",
    "            correct_predictions+= torch.sum(predicted_labels==labels)\n",
    "    average_test_loss = test_loss/len(test_loader)\n",
    "    accuracy=correct_predictions/len(test_dataset)\n",
    "    print(f'Test Loss: {average_test_loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy= accuracy\n",
    "        patience_counter=0\n",
    "        torch.save({'model.state_dict()': model.state_dict(), 'optimizer.state_dict()': optimizer.state_dict()}, 'model.pt')\n",
    "        patience_counter=0\n",
    "    else:\n",
    "        patience_counter+=1\n",
    "    if patience_counter>=patience:\n",
    "        print('Early stopping Triggered')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e9e5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2- Balancing train dataset for potential model improvement and use same LLama pre-trained model for fine-tuning and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40d557f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     11229\n",
       "2     25915\n",
       "4     35784\n",
       "3     42988\n",
       "0     90890\n",
       "1    105800\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = train['label'].value_counts()\n",
    "# Sort the counts from smallest to largest\n",
    "class_counts_sorted = class_counts.sort_values()\n",
    "class_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e9216d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train=train.iloc[:,:768]\n",
    "y_train=train['label']\n",
    "sm=SMOTE(random_state=25, n_jobs=-1, k_neighbors=5)\n",
    "X_train_sm, y_train_sm= sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a20055aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(634800, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60162a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    105800\n",
       "1    105800\n",
       "2    105800\n",
       "3    105800\n",
       "4    105800\n",
       "5    105800\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sm.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b16abb49-9fdd-4db6-aa68-128ae253ba2f",
   "metadata": {},
   "source": [
    "del train, X_train, y_train, class_counts, class_counts_sorted, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35fced0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_embeddings= np.stack(X_train_sm.iloc[:,:768].values)\n",
    "train_labels= y_train_sm.values\n",
    "train_dataset= EmbeddedDataset(train_embeddings, train_labels)\n",
    "train_loader= DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb906995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embeddings= np.stack(test.iloc[:,:768].values)\n",
    "test_labels= test['label'].values\n",
    "test_dataset= EmbeddedDataset(test_embeddings, test_labels)\n",
    "test_loader= DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adf2736c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_labels= len(train['label'].unique())\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac2f1e32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fed39c1ef3741b394882b13e427f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/535 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e0b670895d406bb188bfa05106751e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at skeskinen/llama-lite-134m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AdamW\n",
    "model_name='skeskinen/llama-lite-134m'\n",
    "model=LlamaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "optimizer= AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3187535-6de4-442b-93d1-115fb9990ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((312606, 771), (104203, 771))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape , test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "880f1921-b7c0-4afa-81b0-b51261fcb042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6910204766863348\n",
      "Test Loss: 0.6035168456884981, Accuracy: 0.767338752746582\n",
      "Epoch: 2, Loss: 0.4778885061117259\n",
      "Test Loss: 0.5801429836662269, Accuracy: 0.7747953534126282\n",
      "Epoch: 3, Loss: 0.31834184839205326\n",
      "Test Loss: 0.6402754418323376, Accuracy: 0.7651699185371399\n",
      "Epoch: 4, Loss: 0.20605498998503288\n",
      "Test Loss: 0.7783686681759138, Accuracy: 0.7620317935943604\n",
      "Epoch: 5, Loss: 0.15999099656915108\n",
      "Test Loss: 0.8979340281954572, Accuracy: 0.7577421069145203\n",
      "Early stopping Triggered\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "best_test_loss=float('inf')\n",
    "patience=3\n",
    "patience_counter=0\n",
    "best_accuracy=-1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss= 0\n",
    "    for batch in train_loader:\n",
    "        embeddings=batch[0].to(device)\n",
    "        labels=batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "        loss= outputs.loss\n",
    "        total_loss+= loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss= total_loss / len(train_loader)\n",
    "    print (f'Epoch: {epoch+1}, Loss: {average_loss}')\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss= 0\n",
    "    correct_predictions= 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            embeddings=batch[0].to(device)\n",
    "            labels=batch[1].to(device)\n",
    "            \n",
    "            outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "            test_loss+= outputs.loss.item()\n",
    "            \n",
    "            logits= outputs.logits\n",
    "            _, predicted_labels= torch.max(logits, dim=1)\n",
    "            correct_predictions+= torch.sum(predicted_labels==labels)\n",
    "    average_test_loss = test_loss/len(test_loader)\n",
    "    accuracy=correct_predictions/len(test_dataset)\n",
    "    print(f'Test Loss: {average_test_loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy= accuracy\n",
    "        patience_counter=0\n",
    "        torch.save({'model.state_dict()': model.state_dict(), 'optimizer.state_dict()': optimizer.state_dict()}, 'model_unbalanced.pt')\n",
    "        patience_counter=0\n",
    "    else:\n",
    "        patience_counter+=1\n",
    "    if patience_counter>=patience:\n",
    "        print('Early stopping Triggered')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b64cd9-5205-4a42-ae04-3534f688ec80",
   "metadata": {},
   "source": [
    "### Balancing data slightly improved the performance of the model, which cause increase in accuracy from 75.6% on epoch 2 to 77.5% again on epoch 2 when using balanced data."
   ]
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
