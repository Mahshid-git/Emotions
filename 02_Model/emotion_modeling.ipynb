{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef859c0-b3fc-4cc0-8415-da37beb4a933",
   "metadata": {},
   "source": [
    "### Data is cleaned and embedded using MPNet model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706acbe0-b149-4a9d-b108-84786e15dd37",
   "metadata": {},
   "source": [
    "## Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42975b5-158e-4649-bd4c-cace2d037168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5797c62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embd_data=pd.read_csv('/domino/datasets/local/SCM_SC/text_mpnet_embed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89501b16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416809, 771)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "618dbe25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = embd_data.drop('label', axis=1)\n",
    "y = embd_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90d1e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving Train and test data for later use in all modeling efforts\n",
    "train.to_csv('/domino/datasets/local/SCM_SC/train_df.csv')\n",
    "test.to_csv('/domino/datasets/local/SCM_SC/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df117a87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>...</th>\n",
       "      <th>c_761</th>\n",
       "      <th>c_762</th>\n",
       "      <th>c_763</th>\n",
       "      <th>c_764</th>\n",
       "      <th>c_765</th>\n",
       "      <th>c_766</th>\n",
       "      <th>c_767</th>\n",
       "      <th>text_WO_stopwords</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342462</th>\n",
       "      <td>0.005049</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>-0.033797</td>\n",
       "      <td>-0.011138</td>\n",
       "      <td>-0.024820</td>\n",
       "      <td>0.018492</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>-0.017906</td>\n",
       "      <td>0.047792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.024708</td>\n",
       "      <td>-0.039280</td>\n",
       "      <td>-0.020336</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>-0.060829</td>\n",
       "      <td>-0.017347</td>\n",
       "      <td>feel im torture slowly today</td>\n",
       "      <td>i feel as if im being tortured very slowly today</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87990</th>\n",
       "      <td>-0.039426</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>-0.022700</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>-0.009668</td>\n",
       "      <td>-0.055998</td>\n",
       "      <td>0.037705</td>\n",
       "      <td>-0.007174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043236</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.007713</td>\n",
       "      <td>-0.046003</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>-0.012333</td>\n",
       "      <td>ill feel really bad</td>\n",
       "      <td>ill feel really bad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381594</th>\n",
       "      <td>0.027580</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>-0.028398</td>\n",
       "      <td>-0.019655</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>-0.028050</td>\n",
       "      <td>-0.067037</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>-0.047099</td>\n",
       "      <td>0.023724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>-0.061392</td>\n",
       "      <td>-0.028997</td>\n",
       "      <td>0.022805</td>\n",
       "      <td>0.026034</td>\n",
       "      <td>-0.006839</td>\n",
       "      <td>feel content whatever news tell know rise chal...</td>\n",
       "      <td>i feel content with whatever news we are told ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208217</th>\n",
       "      <td>-0.002705</td>\n",
       "      <td>0.047667</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.026343</td>\n",
       "      <td>0.015890</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>-0.091406</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.029813</td>\n",
       "      <td>0.026739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031983</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.035115</td>\n",
       "      <td>0.008799</td>\n",
       "      <td>0.015763</td>\n",
       "      <td>0.038264</td>\n",
       "      <td>-0.037314</td>\n",
       "      <td>love feel invigorate arrive work</td>\n",
       "      <td>i love feeling invigorated when i arrive at work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376976</th>\n",
       "      <td>0.014338</td>\n",
       "      <td>-0.047544</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>-0.060139</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.072453</td>\n",
       "      <td>0.019001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019103</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>-0.013725</td>\n",
       "      <td>-0.008387</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>feel acceptable use first many polish get rece...</td>\n",
       "      <td>i feel is acceptable i used the first of many ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             c_0       c_1       c_2       c_3       c_4       c_5       c_6  \\\n",
       "342462  0.005049 -0.004521 -0.033797 -0.011138 -0.024820  0.018492 -0.015220   \n",
       "87990  -0.039426 -0.004666 -0.022700  0.006759 -0.026521  0.036558 -0.009668   \n",
       "381594  0.027580  0.060547 -0.028398 -0.019655  0.017900 -0.028050 -0.067037   \n",
       "208217 -0.002705  0.047667 -0.001580 -0.026343  0.015890  0.024639 -0.091406   \n",
       "376976  0.014338 -0.047544  0.025093  0.011402 -0.006114 -0.007457 -0.060139   \n",
       "\n",
       "             c_7       c_8       c_9  ...     c_761     c_762     c_763  \\\n",
       "342462  0.012329 -0.017906  0.047792  ...  0.021197  0.024708 -0.039280   \n",
       "87990  -0.055998  0.037705 -0.007174  ... -0.043236  0.020760  0.006370   \n",
       "381594  0.006156 -0.047099  0.023724  ...  0.000482  0.009941 -0.061392   \n",
       "208217  0.014512  0.029813  0.026739  ...  0.031983 -0.009349 -0.035115   \n",
       "376976 -0.041030 -0.072453  0.019001  ... -0.019103  0.007613 -0.013725   \n",
       "\n",
       "           c_764     c_765     c_766     c_767  \\\n",
       "342462 -0.020336  0.022013 -0.060829 -0.017347   \n",
       "87990   0.007713 -0.046003 -0.000759 -0.012333   \n",
       "381594 -0.028997  0.022805  0.026034 -0.006839   \n",
       "208217  0.008799  0.015763  0.038264 -0.037314   \n",
       "376976 -0.008387  0.000549  0.007308  0.002215   \n",
       "\n",
       "                                        text_WO_stopwords  \\\n",
       "342462                       feel im torture slowly today   \n",
       "87990                                 ill feel really bad   \n",
       "381594  feel content whatever news tell know rise chal...   \n",
       "208217                   love feel invigorate arrive work   \n",
       "376976  feel acceptable use first many polish get rece...   \n",
       "\n",
       "                                                     text  label  \n",
       "342462   i feel as if im being tortured very slowly today      4  \n",
       "87990                                 ill feel really bad      0  \n",
       "381594  i feel content with whatever news we are told ...      1  \n",
       "208217   i love feeling invigorated when i arrive at work      1  \n",
       "376976  i feel is acceptable i used the first of many ...      1  \n",
       "\n",
       "[5 rows x 771 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973a58a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Requirement already satisfied: imbalanced-learn in /home/ubuntu/scm/lib/python3.8/site-packages (from imblearn) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/ubuntu/.local/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=0.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/ubuntu/scm/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ubuntu/scm/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Installing collected packages: imblearn\n",
      "Successfully installed imblearn-0.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/ubuntu/scm/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/scm/lib/python3.8/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Sentence_Transformers in /home/ubuntu/scm/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (4.30.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (from Sentence_Transformers) (1.22.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (0.22.1)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (1.8.1)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from Sentence_Transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (2.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/scm/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->Sentence_Transformers) (23.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from torch>=1.6.0->Sentence_Transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/scm/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->Sentence_Transformers) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/scm/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->Sentence_Transformers) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/ubuntu/scm/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->Sentence_Transformers) (3.26.4)\n",
      "Requirement already satisfied: lit in /home/ubuntu/scm/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->Sentence_Transformers) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->Sentence_Transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->Sentence_Transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->Sentence_Transformers) (0.3.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk->Sentence_Transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/scm/lib/python3.8/site-packages (from nltk->Sentence_Transformers) (1.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from torchvision->Sentence_Transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/scm/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->Sentence_Transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->Sentence_Transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->Sentence_Transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->Sentence_Transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/scm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->Sentence_Transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/scm/lib/python3.8/site-packages (from sympy->torch>=1.6.0->Sentence_Transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install Sentence_Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e745b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1- Using Embedded data from MPNet model and LLama Lite pre-trained model as classifier for emotion classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2b8849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords#, PlainTextCorpusReader\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, balanced_accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a468b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbeddedDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    def __getitem__(self, idx):\n",
    "        embedding=self.embeddings[idx]\n",
    "        embedding=torch.tensor(embedding, dtype=torch.float32)\n",
    "        embedding=embedding.unsqueeze(0)\n",
    "        return embedding, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d7f285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /mnt/code/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a15925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "135ee28e",
   "metadata": {},
   "source": [
    "# No need to run this cell as our labels is already encoded\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(train['label'])\n",
    "train['label_coded'] = label_encoder.transform(train['label'])\n",
    "test['label_coded'] = label_encoder.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e107c367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  312606 \n",
      "test:  104203\n"
     ]
    }
   ],
   "source": [
    "print('train: ', len(train), '\\ntest: ', len(test)) #, '\\nval: ', len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd869a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['label'] = train['label'].astype('category')\n",
    "test['label'] = test['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a6f56d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c_0                   float64\n",
       "c_1                   float64\n",
       "c_2                   float64\n",
       "c_3                   float64\n",
       "c_4                   float64\n",
       "                       ...   \n",
       "c_766                 float64\n",
       "c_767                 float64\n",
       "text_WO_stopwords      object\n",
       "text                   object\n",
       "label                category\n",
       "Length: 771, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff49ba1",
   "metadata": {},
   "source": [
    "## Prepare model input for model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f81e5fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_embeddings= np.stack(train.iloc[:,:768].values)\n",
    "train_labels= train['label'].values\n",
    "train_dataset= EmbeddedDataset(train_embeddings, train_labels)\n",
    "train_loader= DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cc74985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embeddings= np.stack(test.iloc[:,:768].values)\n",
    "test_labels= test['label'].values\n",
    "test_dataset= EmbeddedDataset(test_embeddings, test_labels)\n",
    "test_loader= DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a7ef960-e379-4030-add2-9a0f0b479489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_labels= len(train['label'].unique())\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6767abae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at skeskinen/llama-lite-134m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AdamW\n",
    "model_name='skeskinen/llama-lite-134m'\n",
    "model=LlamaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "optimizer= AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c0b681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.47116596780176606\n",
      "Test Loss: 0.6818288690473404, Accuracy: 0.750822901725769\n",
      "Epoch: 2, Loss: 0.25431708495673394\n",
      "Test Loss: 0.7496792136891488, Accuracy: 0.7564273476600647\n",
      "Epoch: 3, Loss: 0.1716895796681544\n",
      "Test Loss: 0.8472883035251699, Accuracy: 0.7544216513633728\n",
      "Epoch: 4, Loss: 0.13620078459216822\n",
      "Test Loss: 0.9292120997525432, Accuracy: 0.7511683702468872\n",
      "Epoch: 5, Loss: 0.12082830793036509\n",
      "Test Loss: 0.9896049231839327, Accuracy: 0.7497192621231079\n",
      "Early stopping Triggered\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "best_test_loss=float('inf')\n",
    "patience=3\n",
    "patience_counter=0\n",
    "best_accuracy=-1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss= 0\n",
    "    for batch in train_loader:\n",
    "        embeddings=batch[0].to(device)\n",
    "        labels=batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "        loss= outputs.loss\n",
    "        total_loss+= loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss= total_loss / len(train_loader)\n",
    "    print (f'Epoch: {epoch+1}, Loss: {average_loss}')\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss= 0\n",
    "    correct_predictions= 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            embeddings=batch[0].to(device)\n",
    "            labels=batch[1].to(device)\n",
    "            \n",
    "            outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "            test_loss+= outputs.loss.item()\n",
    "            \n",
    "            logits= outputs.logits\n",
    "            _, predicted_labels= torch.max(logits, dim=1)\n",
    "            correct_predictions+= torch.sum(predicted_labels==labels)\n",
    "    average_test_loss = test_loss/len(test_loader)\n",
    "    accuracy=correct_predictions/len(test_dataset)\n",
    "    print(f'Test Loss: {average_test_loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy= accuracy\n",
    "        patience_counter=0\n",
    "        torch.save({'model.state_dict()': model.state_dict(), 'optimizer.state_dict()': optimizer.state_dict()}, 'model.pt')\n",
    "        patience_counter=0\n",
    "    else:\n",
    "        patience_counter+=1\n",
    "    if patience_counter>=patience:\n",
    "        print('Early stopping Triggered')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e9e5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2- Balancing train dataset for potential model improvement and use same LLama pre-trained model for fine-tuning and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40d557f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     11229\n",
       "2     25915\n",
       "4     35784\n",
       "3     42988\n",
       "0     90890\n",
       "1    105800\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = train['label'].value_counts()\n",
    "# Sort the counts from smallest to largest\n",
    "class_counts_sorted = class_counts.sort_values()\n",
    "class_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e9216d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train=train.iloc[:,:768]\n",
    "y_train=train['label']\n",
    "sm=SMOTE(random_state=25, n_jobs=-1, k_neighbors=5)\n",
    "X_train_sm, y_train_sm= sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a20055aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(634800, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60162a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    105800\n",
       "1    105800\n",
       "2    105800\n",
       "3    105800\n",
       "4    105800\n",
       "5    105800\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sm.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b16abb49-9fdd-4db6-aa68-128ae253ba2f",
   "metadata": {},
   "source": [
    "del train, X_train, y_train, class_counts, class_counts_sorted, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35fced0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_embeddings= np.stack(X_train_sm.iloc[:,:768].values)\n",
    "train_labels= y_train_sm.values\n",
    "train_dataset= EmbeddedDataset(train_embeddings, train_labels)\n",
    "train_loader= DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb906995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embeddings= np.stack(test.iloc[:,:768].values)\n",
    "test_labels= test['label'].values\n",
    "test_dataset= EmbeddedDataset(test_embeddings, test_labels)\n",
    "test_loader= DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adf2736c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_labels= len(train['label'].unique())\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac2f1e32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fed39c1ef3741b394882b13e427f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/535 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e0b670895d406bb188bfa05106751e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at skeskinen/llama-lite-134m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AdamW\n",
    "model_name='skeskinen/llama-lite-134m'\n",
    "model=LlamaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "optimizer= AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3187535-6de4-442b-93d1-115fb9990ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((312606, 771), (104203, 771))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape , test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "880f1921-b7c0-4afa-81b0-b51261fcb042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6910204766863348\n",
      "Test Loss: 0.6035168456884981, Accuracy: 0.767338752746582\n",
      "Epoch: 2, Loss: 0.4778885061117259\n",
      "Test Loss: 0.5801429836662269, Accuracy: 0.7747953534126282\n",
      "Epoch: 3, Loss: 0.31834184839205326\n",
      "Test Loss: 0.6402754418323376, Accuracy: 0.7651699185371399\n",
      "Epoch: 4, Loss: 0.20605498998503288\n",
      "Test Loss: 0.7783686681759138, Accuracy: 0.7620317935943604\n",
      "Epoch: 5, Loss: 0.15999099656915108\n",
      "Test Loss: 0.8979340281954572, Accuracy: 0.7577421069145203\n",
      "Early stopping Triggered\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "best_test_loss=float('inf')\n",
    "patience=3\n",
    "patience_counter=0\n",
    "best_accuracy=-1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss= 0\n",
    "    for batch in train_loader:\n",
    "        embeddings=batch[0].to(device)\n",
    "        labels=batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "        loss= outputs.loss\n",
    "        total_loss+= loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss= total_loss / len(train_loader)\n",
    "    print (f'Epoch: {epoch+1}, Loss: {average_loss}')\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss= 0\n",
    "    correct_predictions= 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            embeddings=batch[0].to(device)\n",
    "            labels=batch[1].to(device)\n",
    "            \n",
    "            outputs= model(inputs_embeds=embeddings, labels=labels)\n",
    "            test_loss+= outputs.loss.item()\n",
    "            \n",
    "            logits= outputs.logits\n",
    "            _, predicted_labels= torch.max(logits, dim=1)\n",
    "            correct_predictions+= torch.sum(predicted_labels==labels)\n",
    "    average_test_loss = test_loss/len(test_loader)\n",
    "    accuracy=correct_predictions/len(test_dataset)\n",
    "    print(f'Test Loss: {average_test_loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy= accuracy\n",
    "        patience_counter=0\n",
    "        torch.save({'model.state_dict()': model.state_dict(), 'optimizer.state_dict()': optimizer.state_dict()}, 'model_unbalanced.pt')\n",
    "        patience_counter=0\n",
    "    else:\n",
    "        patience_counter+=1\n",
    "    if patience_counter>=patience:\n",
    "        print('Early stopping Triggered')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b64cd9-5205-4a42-ae04-3534f688ec80",
   "metadata": {},
   "source": [
    "### Balancing data slightly improved the performance of the model, which cause increase in accuracy from 75.6% on epoch 2 to 77.5% again on epoch 2 when using balanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ae779",
   "metadata": {},
   "source": [
    "# 3- Another Method Keras tokenizer and Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00c0ff5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 23:55:56.076835: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcc6591-2766-4a8c-b5de-05dfb5fd5783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading Train and Test clean data which is partitioned and saved for later uses previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38f166d-4c7f-4d01-8c95-2bbb4ed293e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing data with clean text column (\"text_WO_stopwords) and it also have embedded features too , just in case we use it for modeling\n",
    "train=pd.read_csv('/domino/datasets/local/SCM_SC/train_df.csv')\n",
    "test=pd.read_csv('/domino/datasets/local/SCM_SC/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fe40ee-836d-4185-b293-4cde20bba929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>...</th>\n",
       "      <th>c_761</th>\n",
       "      <th>c_762</th>\n",
       "      <th>c_763</th>\n",
       "      <th>c_764</th>\n",
       "      <th>c_765</th>\n",
       "      <th>c_766</th>\n",
       "      <th>c_767</th>\n",
       "      <th>text_WO_stopwords</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019394</td>\n",
       "      <td>-0.044919</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.017203</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.066090</td>\n",
       "      <td>-0.049987</td>\n",
       "      <td>-0.011413</td>\n",
       "      <td>0.036748</td>\n",
       "      <td>0.019023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000739</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>-0.109465</td>\n",
       "      <td>-0.013429</td>\n",
       "      <td>0.031202</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>0.030161</td>\n",
       "      <td>rin dahil bakit ako magtatanim ng sama ng loob...</td>\n",
       "      <td>i rin dahil bakit ako magtatanim ng sama ng lo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021275</td>\n",
       "      <td>-0.005803</td>\n",
       "      <td>-0.015457</td>\n",
       "      <td>-0.018007</td>\n",
       "      <td>-0.004438</td>\n",
       "      <td>0.047791</td>\n",
       "      <td>-0.081813</td>\n",
       "      <td>0.030755</td>\n",
       "      <td>-0.032801</td>\n",
       "      <td>-0.027810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031838</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>-0.027941</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>-0.039903</td>\n",
       "      <td>0.025485</td>\n",
       "      <td>still feel bite distraught try get</td>\n",
       "      <td>i am still feeling a bit distraught but i have...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029161</td>\n",
       "      <td>0.051610</td>\n",
       "      <td>-0.010187</td>\n",
       "      <td>0.012589</td>\n",
       "      <td>-0.000941</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>-0.051471</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>-0.054751</td>\n",
       "      <td>-0.011609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027453</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>0.045774</td>\n",
       "      <td>-0.027437</td>\n",
       "      <td>-0.005116</td>\n",
       "      <td>0.042347</td>\n",
       "      <td>-0.007992</td>\n",
       "      <td>scoop card gift wrap feel smug part group gift...</td>\n",
       "      <td>i scooped up a card and gift wrap feeling smug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.035654</td>\n",
       "      <td>0.024396</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.020141</td>\n",
       "      <td>0.012857</td>\n",
       "      <td>-0.051097</td>\n",
       "      <td>-0.053518</td>\n",
       "      <td>-0.021370</td>\n",
       "      <td>-0.034440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049417</td>\n",
       "      <td>-0.009656</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>-0.043896</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.042334</td>\n",
       "      <td>-0.026798</td>\n",
       "      <td>feel important</td>\n",
       "      <td>i feel that is so important</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031208</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>-0.033559</td>\n",
       "      <td>0.057609</td>\n",
       "      <td>-0.021166</td>\n",
       "      <td>-0.019349</td>\n",
       "      <td>-0.044515</td>\n",
       "      <td>-0.010385</td>\n",
       "      <td>-0.010897</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049952</td>\n",
       "      <td>-0.008722</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>-0.047218</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>-0.045528</td>\n",
       "      <td>tell expectation important day would feel spec...</td>\n",
       "      <td>i told him that my expectation for that very i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104198</th>\n",
       "      <td>0.027633</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>-0.001323</td>\n",
       "      <td>-0.051901</td>\n",
       "      <td>-0.010680</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>-0.028517</td>\n",
       "      <td>-0.030698</td>\n",
       "      <td>0.053657</td>\n",
       "      <td>-0.050739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.042012</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>-0.030643</td>\n",
       "      <td>0.071695</td>\n",
       "      <td>-0.016831</td>\n",
       "      <td>feel dirty remember way use think</td>\n",
       "      <td>i feel dirty just remembering the way i used t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104199</th>\n",
       "      <td>-0.044641</td>\n",
       "      <td>0.050187</td>\n",
       "      <td>-0.019373</td>\n",
       "      <td>-0.018147</td>\n",
       "      <td>0.026715</td>\n",
       "      <td>0.011591</td>\n",
       "      <td>-0.039571</td>\n",
       "      <td>0.012069</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>0.020797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016689</td>\n",
       "      <td>-0.003244</td>\n",
       "      <td>-0.024648</td>\n",
       "      <td>-0.020703</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>0.099140</td>\n",
       "      <td>-0.021172</td>\n",
       "      <td>feel bless short years</td>\n",
       "      <td>i feel blessed to have had too short years wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104200</th>\n",
       "      <td>-0.019972</td>\n",
       "      <td>0.094170</td>\n",
       "      <td>0.015813</td>\n",
       "      <td>-0.039249</td>\n",
       "      <td>0.029252</td>\n",
       "      <td>0.026102</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.015672</td>\n",
       "      <td>0.072358</td>\n",
       "      <td>0.025073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027735</td>\n",
       "      <td>-0.008830</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>-0.014148</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>0.053207</td>\n",
       "      <td>-0.007986</td>\n",
       "      <td>feel intelligent additional years</td>\n",
       "      <td>i feel intelligent for for an additional years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104201</th>\n",
       "      <td>0.049425</td>\n",
       "      <td>0.008892</td>\n",
       "      <td>-0.011874</td>\n",
       "      <td>-0.026436</td>\n",
       "      <td>-0.023583</td>\n",
       "      <td>0.035841</td>\n",
       "      <td>-0.031572</td>\n",
       "      <td>0.064218</td>\n",
       "      <td>-0.094790</td>\n",
       "      <td>0.014435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028396</td>\n",
       "      <td>-0.013129</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.055631</td>\n",
       "      <td>-0.029546</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>im feel bite less wimpy still pastel</td>\n",
       "      <td>im feeling a bit less wimpy about them but sti...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104202</th>\n",
       "      <td>-0.050315</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>-0.033597</td>\n",
       "      <td>-0.017755</td>\n",
       "      <td>-0.018156</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.051800</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>-0.019257</td>\n",
       "      <td>0.014847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>-0.042766</td>\n",
       "      <td>-0.035969</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>feel sentimental know blip family timeline</td>\n",
       "      <td>i feel so sentimental knowing this is just a b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104203 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             c_0       c_1       c_2       c_3       c_4       c_5       c_6  \\\n",
       "0       0.019394 -0.044919  0.000026  0.017203  0.038961  0.066090 -0.049987   \n",
       "1       0.021275 -0.005803 -0.015457 -0.018007 -0.004438  0.047791 -0.081813   \n",
       "2       0.029161  0.051610 -0.010187  0.012589 -0.000941  0.013400 -0.051471   \n",
       "3       0.029598  0.035654  0.024396  0.022355  0.020141  0.012857 -0.051097   \n",
       "4       0.031208  0.063900 -0.033559  0.057609 -0.021166 -0.019349 -0.044515   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "104198  0.027633  0.025179 -0.001323 -0.051901 -0.010680  0.006089 -0.028517   \n",
       "104199 -0.044641  0.050187 -0.019373 -0.018147  0.026715  0.011591 -0.039571   \n",
       "104200 -0.019972  0.094170  0.015813 -0.039249  0.029252  0.026102  0.013029   \n",
       "104201  0.049425  0.008892 -0.011874 -0.026436 -0.023583  0.035841 -0.031572   \n",
       "104202 -0.050315  0.016274 -0.033597 -0.017755 -0.018156  0.000046 -0.051800   \n",
       "\n",
       "             c_7       c_8       c_9  ...     c_761     c_762     c_763  \\\n",
       "0      -0.011413  0.036748  0.019023  ... -0.000739  0.003633 -0.109465   \n",
       "1       0.030755 -0.032801 -0.027810  ... -0.031838  0.027748 -0.027941   \n",
       "2       0.022667 -0.054751 -0.011609  ...  0.027453 -0.002714  0.045774   \n",
       "3      -0.053518 -0.021370 -0.034440  ...  0.049417 -0.009656 -0.074797   \n",
       "4      -0.010385 -0.010897  0.010456  ...  0.049952 -0.008722  0.004873   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "104198 -0.030698  0.053657 -0.050739  ... -0.000989  0.003046  0.042012   \n",
       "104199  0.012069  0.004874  0.020797  ... -0.016689 -0.003244 -0.024648   \n",
       "104200  0.015672  0.072358  0.025073  ... -0.027735 -0.008830  0.016110   \n",
       "104201  0.064218 -0.094790  0.014435  ... -0.028396 -0.013129  0.038700   \n",
       "104202  0.001080 -0.019257  0.014847  ...  0.013811 -0.006754 -0.042766   \n",
       "\n",
       "           c_764     c_765     c_766     c_767  \\\n",
       "0      -0.013429  0.031202 -0.058474  0.030161   \n",
       "1       0.000003  0.014574 -0.039903  0.025485   \n",
       "2      -0.027437 -0.005116  0.042347 -0.007992   \n",
       "3      -0.043896  0.000957  0.042334 -0.026798   \n",
       "4      -0.047218  0.007098  0.015758 -0.045528   \n",
       "...          ...       ...       ...       ...   \n",
       "104198  0.003837 -0.030643  0.071695 -0.016831   \n",
       "104199 -0.020703  0.025322  0.099140 -0.021172   \n",
       "104200 -0.014148  0.018982  0.053207 -0.007986   \n",
       "104201  0.021196  0.055631 -0.029546  0.021097   \n",
       "104202 -0.035969  0.043652  0.004725  0.002750   \n",
       "\n",
       "                                        text_WO_stopwords  \\\n",
       "0       rin dahil bakit ako magtatanim ng sama ng loob...   \n",
       "1                      still feel bite distraught try get   \n",
       "2       scoop card gift wrap feel smug part group gift...   \n",
       "3                                          feel important   \n",
       "4       tell expectation important day would feel spec...   \n",
       "...                                                   ...   \n",
       "104198                  feel dirty remember way use think   \n",
       "104199                             feel bless short years   \n",
       "104200                  feel intelligent additional years   \n",
       "104201               im feel bite less wimpy still pastel   \n",
       "104202         feel sentimental know blip family timeline   \n",
       "\n",
       "                                                     text  label  \n",
       "0       i rin dahil bakit ako magtatanim ng sama ng lo...      3  \n",
       "1       i am still feeling a bit distraught but i have...      4  \n",
       "2       i scooped up a card and gift wrap feeling smug...      1  \n",
       "3                             i feel that is so important      1  \n",
       "4       i told him that my expectation for that very i...      1  \n",
       "...                                                   ...    ...  \n",
       "104198  i feel dirty just remembering the way i used t...      0  \n",
       "104199  i feel blessed to have had too short years wit...      1  \n",
       "104200     i feel intelligent for for an additional years      1  \n",
       "104201  im feeling a bit less wimpy about them but sti...      4  \n",
       "104202  i feel so sentimental knowing this is just a b...      0  \n",
       "\n",
       "[104203 rows x 771 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to remove the first column\n",
    "train = train.drop(train.columns[0], axis=1)\n",
    "test = test.drop(test.columns[0], axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "827ecb26-4364-4fe4-93d5-8549bb32d1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>...</th>\n",
       "      <th>c_761</th>\n",
       "      <th>c_762</th>\n",
       "      <th>c_763</th>\n",
       "      <th>c_764</th>\n",
       "      <th>c_765</th>\n",
       "      <th>c_766</th>\n",
       "      <th>c_767</th>\n",
       "      <th>text_WO_stopwords</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019394</td>\n",
       "      <td>-0.044919</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.017203</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.066090</td>\n",
       "      <td>-0.049987</td>\n",
       "      <td>-0.011413</td>\n",
       "      <td>0.036748</td>\n",
       "      <td>0.019023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000739</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>-0.109465</td>\n",
       "      <td>-0.013429</td>\n",
       "      <td>0.031202</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>0.030161</td>\n",
       "      <td>rin dahil bakit ako magtatanim ng sama ng loob...</td>\n",
       "      <td>i rin dahil bakit ako magtatanim ng sama ng lo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021275</td>\n",
       "      <td>-0.005803</td>\n",
       "      <td>-0.015457</td>\n",
       "      <td>-0.018007</td>\n",
       "      <td>-0.004438</td>\n",
       "      <td>0.047791</td>\n",
       "      <td>-0.081813</td>\n",
       "      <td>0.030755</td>\n",
       "      <td>-0.032801</td>\n",
       "      <td>-0.027810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031838</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>-0.027941</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>-0.039903</td>\n",
       "      <td>0.025485</td>\n",
       "      <td>still feel bite distraught try get</td>\n",
       "      <td>i am still feeling a bit distraught but i have...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029161</td>\n",
       "      <td>0.051610</td>\n",
       "      <td>-0.010187</td>\n",
       "      <td>0.012589</td>\n",
       "      <td>-0.000941</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>-0.051471</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>-0.054751</td>\n",
       "      <td>-0.011609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027453</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>0.045774</td>\n",
       "      <td>-0.027437</td>\n",
       "      <td>-0.005116</td>\n",
       "      <td>0.042347</td>\n",
       "      <td>-0.007992</td>\n",
       "      <td>scoop card gift wrap feel smug part group gift...</td>\n",
       "      <td>i scooped up a card and gift wrap feeling smug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.035654</td>\n",
       "      <td>0.024396</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.020141</td>\n",
       "      <td>0.012857</td>\n",
       "      <td>-0.051097</td>\n",
       "      <td>-0.053518</td>\n",
       "      <td>-0.021370</td>\n",
       "      <td>-0.034440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049417</td>\n",
       "      <td>-0.009656</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>-0.043896</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.042334</td>\n",
       "      <td>-0.026798</td>\n",
       "      <td>feel important</td>\n",
       "      <td>i feel that is so important</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031208</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>-0.033559</td>\n",
       "      <td>0.057609</td>\n",
       "      <td>-0.021166</td>\n",
       "      <td>-0.019349</td>\n",
       "      <td>-0.044515</td>\n",
       "      <td>-0.010385</td>\n",
       "      <td>-0.010897</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049952</td>\n",
       "      <td>-0.008722</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>-0.047218</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>-0.045528</td>\n",
       "      <td>tell expectation important day would feel spec...</td>\n",
       "      <td>i told him that my expectation for that very i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104198</th>\n",
       "      <td>0.027633</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>-0.001323</td>\n",
       "      <td>-0.051901</td>\n",
       "      <td>-0.010680</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>-0.028517</td>\n",
       "      <td>-0.030698</td>\n",
       "      <td>0.053657</td>\n",
       "      <td>-0.050739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.042012</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>-0.030643</td>\n",
       "      <td>0.071695</td>\n",
       "      <td>-0.016831</td>\n",
       "      <td>feel dirty remember way use think</td>\n",
       "      <td>i feel dirty just remembering the way i used t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104199</th>\n",
       "      <td>-0.044641</td>\n",
       "      <td>0.050187</td>\n",
       "      <td>-0.019373</td>\n",
       "      <td>-0.018147</td>\n",
       "      <td>0.026715</td>\n",
       "      <td>0.011591</td>\n",
       "      <td>-0.039571</td>\n",
       "      <td>0.012069</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>0.020797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016689</td>\n",
       "      <td>-0.003244</td>\n",
       "      <td>-0.024648</td>\n",
       "      <td>-0.020703</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>0.099140</td>\n",
       "      <td>-0.021172</td>\n",
       "      <td>feel bless short years</td>\n",
       "      <td>i feel blessed to have had too short years wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104200</th>\n",
       "      <td>-0.019972</td>\n",
       "      <td>0.094170</td>\n",
       "      <td>0.015813</td>\n",
       "      <td>-0.039249</td>\n",
       "      <td>0.029252</td>\n",
       "      <td>0.026102</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.015672</td>\n",
       "      <td>0.072358</td>\n",
       "      <td>0.025073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027735</td>\n",
       "      <td>-0.008830</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>-0.014148</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>0.053207</td>\n",
       "      <td>-0.007986</td>\n",
       "      <td>feel intelligent additional years</td>\n",
       "      <td>i feel intelligent for for an additional years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104201</th>\n",
       "      <td>0.049425</td>\n",
       "      <td>0.008892</td>\n",
       "      <td>-0.011874</td>\n",
       "      <td>-0.026436</td>\n",
       "      <td>-0.023583</td>\n",
       "      <td>0.035841</td>\n",
       "      <td>-0.031572</td>\n",
       "      <td>0.064218</td>\n",
       "      <td>-0.094790</td>\n",
       "      <td>0.014435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028396</td>\n",
       "      <td>-0.013129</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.055631</td>\n",
       "      <td>-0.029546</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>im feel bite less wimpy still pastel</td>\n",
       "      <td>im feeling a bit less wimpy about them but sti...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104202</th>\n",
       "      <td>-0.050315</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>-0.033597</td>\n",
       "      <td>-0.017755</td>\n",
       "      <td>-0.018156</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.051800</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>-0.019257</td>\n",
       "      <td>0.014847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>-0.042766</td>\n",
       "      <td>-0.035969</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>feel sentimental know blip family timeline</td>\n",
       "      <td>i feel so sentimental knowing this is just a b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104203 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             c_0       c_1       c_2       c_3       c_4       c_5       c_6  \\\n",
       "0       0.019394 -0.044919  0.000026  0.017203  0.038961  0.066090 -0.049987   \n",
       "1       0.021275 -0.005803 -0.015457 -0.018007 -0.004438  0.047791 -0.081813   \n",
       "2       0.029161  0.051610 -0.010187  0.012589 -0.000941  0.013400 -0.051471   \n",
       "3       0.029598  0.035654  0.024396  0.022355  0.020141  0.012857 -0.051097   \n",
       "4       0.031208  0.063900 -0.033559  0.057609 -0.021166 -0.019349 -0.044515   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "104198  0.027633  0.025179 -0.001323 -0.051901 -0.010680  0.006089 -0.028517   \n",
       "104199 -0.044641  0.050187 -0.019373 -0.018147  0.026715  0.011591 -0.039571   \n",
       "104200 -0.019972  0.094170  0.015813 -0.039249  0.029252  0.026102  0.013029   \n",
       "104201  0.049425  0.008892 -0.011874 -0.026436 -0.023583  0.035841 -0.031572   \n",
       "104202 -0.050315  0.016274 -0.033597 -0.017755 -0.018156  0.000046 -0.051800   \n",
       "\n",
       "             c_7       c_8       c_9  ...     c_761     c_762     c_763  \\\n",
       "0      -0.011413  0.036748  0.019023  ... -0.000739  0.003633 -0.109465   \n",
       "1       0.030755 -0.032801 -0.027810  ... -0.031838  0.027748 -0.027941   \n",
       "2       0.022667 -0.054751 -0.011609  ...  0.027453 -0.002714  0.045774   \n",
       "3      -0.053518 -0.021370 -0.034440  ...  0.049417 -0.009656 -0.074797   \n",
       "4      -0.010385 -0.010897  0.010456  ...  0.049952 -0.008722  0.004873   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "104198 -0.030698  0.053657 -0.050739  ... -0.000989  0.003046  0.042012   \n",
       "104199  0.012069  0.004874  0.020797  ... -0.016689 -0.003244 -0.024648   \n",
       "104200  0.015672  0.072358  0.025073  ... -0.027735 -0.008830  0.016110   \n",
       "104201  0.064218 -0.094790  0.014435  ... -0.028396 -0.013129  0.038700   \n",
       "104202  0.001080 -0.019257  0.014847  ...  0.013811 -0.006754 -0.042766   \n",
       "\n",
       "           c_764     c_765     c_766     c_767  \\\n",
       "0      -0.013429  0.031202 -0.058474  0.030161   \n",
       "1       0.000003  0.014574 -0.039903  0.025485   \n",
       "2      -0.027437 -0.005116  0.042347 -0.007992   \n",
       "3      -0.043896  0.000957  0.042334 -0.026798   \n",
       "4      -0.047218  0.007098  0.015758 -0.045528   \n",
       "...          ...       ...       ...       ...   \n",
       "104198  0.003837 -0.030643  0.071695 -0.016831   \n",
       "104199 -0.020703  0.025322  0.099140 -0.021172   \n",
       "104200 -0.014148  0.018982  0.053207 -0.007986   \n",
       "104201  0.021196  0.055631 -0.029546  0.021097   \n",
       "104202 -0.035969  0.043652  0.004725  0.002750   \n",
       "\n",
       "                                        text_WO_stopwords  \\\n",
       "0       rin dahil bakit ako magtatanim ng sama ng loob...   \n",
       "1                      still feel bite distraught try get   \n",
       "2       scoop card gift wrap feel smug part group gift...   \n",
       "3                                          feel important   \n",
       "4       tell expectation important day would feel spec...   \n",
       "...                                                   ...   \n",
       "104198                  feel dirty remember way use think   \n",
       "104199                             feel bless short years   \n",
       "104200                  feel intelligent additional years   \n",
       "104201               im feel bite less wimpy still pastel   \n",
       "104202         feel sentimental know blip family timeline   \n",
       "\n",
       "                                                     text  label  \n",
       "0       i rin dahil bakit ako magtatanim ng sama ng lo...      3  \n",
       "1       i am still feeling a bit distraught but i have...      4  \n",
       "2       i scooped up a card and gift wrap feeling smug...      1  \n",
       "3                             i feel that is so important      1  \n",
       "4       i told him that my expectation for that very i...      1  \n",
       "...                                                   ...    ...  \n",
       "104198  i feel dirty just remembering the way i used t...      0  \n",
       "104199  i feel blessed to have had too short years wit...      1  \n",
       "104200     i feel intelligent for for an additional years      1  \n",
       "104201  im feeling a bit less wimpy about them but sti...      4  \n",
       "104202  i feel so sentimental knowing this is just a b...      0  \n",
       "\n",
       "[104203 rows x 771 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.fillna('')\n",
    "test = test.fillna('')\n",
    "train.reset_index(drop=True)\n",
    "test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d67d563-1194-4ce1-bf0a-24b36c53b680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>...</th>\n",
       "      <th>c_761</th>\n",
       "      <th>c_762</th>\n",
       "      <th>c_763</th>\n",
       "      <th>c_764</th>\n",
       "      <th>c_765</th>\n",
       "      <th>c_766</th>\n",
       "      <th>c_767</th>\n",
       "      <th>text_WO_stopwords</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005049</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>-0.033797</td>\n",
       "      <td>-0.011138</td>\n",
       "      <td>-0.024820</td>\n",
       "      <td>0.018492</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>-0.017906</td>\n",
       "      <td>0.047792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.024708</td>\n",
       "      <td>-0.039280</td>\n",
       "      <td>-0.020336</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>-0.060829</td>\n",
       "      <td>-0.017347</td>\n",
       "      <td>feel im torture slowly today</td>\n",
       "      <td>i feel as if im being tortured very slowly today</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039426</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>-0.022700</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>-0.009668</td>\n",
       "      <td>-0.055998</td>\n",
       "      <td>0.037705</td>\n",
       "      <td>-0.007174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043236</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.007713</td>\n",
       "      <td>-0.046003</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>-0.012333</td>\n",
       "      <td>ill feel really bad</td>\n",
       "      <td>ill feel really bad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027580</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>-0.028398</td>\n",
       "      <td>-0.019655</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>-0.028050</td>\n",
       "      <td>-0.067037</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>-0.047099</td>\n",
       "      <td>0.023724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>-0.061392</td>\n",
       "      <td>-0.028997</td>\n",
       "      <td>0.022805</td>\n",
       "      <td>0.026034</td>\n",
       "      <td>-0.006839</td>\n",
       "      <td>feel content whatever news tell know rise chal...</td>\n",
       "      <td>i feel content with whatever news we are told ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002705</td>\n",
       "      <td>0.047667</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.026343</td>\n",
       "      <td>0.015890</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>-0.091406</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.029813</td>\n",
       "      <td>0.026739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031983</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.035115</td>\n",
       "      <td>0.008799</td>\n",
       "      <td>0.015763</td>\n",
       "      <td>0.038264</td>\n",
       "      <td>-0.037314</td>\n",
       "      <td>love feel invigorate arrive work</td>\n",
       "      <td>i love feeling invigorated when i arrive at work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014338</td>\n",
       "      <td>-0.047544</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>-0.060139</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.072453</td>\n",
       "      <td>0.019001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019103</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>-0.013725</td>\n",
       "      <td>-0.008387</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>feel acceptable use first many polish get rece...</td>\n",
       "      <td>i feel is acceptable i used the first of many ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312601</th>\n",
       "      <td>-0.020691</td>\n",
       "      <td>-0.014837</td>\n",
       "      <td>-0.003853</td>\n",
       "      <td>-0.028849</td>\n",
       "      <td>-0.007957</td>\n",
       "      <td>0.065683</td>\n",
       "      <td>-0.099857</td>\n",
       "      <td>-0.012396</td>\n",
       "      <td>0.054042</td>\n",
       "      <td>-0.024364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040756</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>-0.043533</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.032682</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>feel terrible course fantastic</td>\n",
       "      <td>i was feeling terrible but of course she did f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312602</th>\n",
       "      <td>0.012106</td>\n",
       "      <td>0.056657</td>\n",
       "      <td>-0.035532</td>\n",
       "      <td>-0.017904</td>\n",
       "      <td>-0.007871</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.011781</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021886</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>-0.027610</td>\n",
       "      <td>0.030307</td>\n",
       "      <td>0.061982</td>\n",
       "      <td>-0.047177</td>\n",
       "      <td>give permission feel vulnerable feel anxious p...</td>\n",
       "      <td>i can give myself permission to feel vulnerabl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312603</th>\n",
       "      <td>-0.013450</td>\n",
       "      <td>-0.008782</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.015963</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>-0.031408</td>\n",
       "      <td>-0.024701</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049028</td>\n",
       "      <td>-0.053441</td>\n",
       "      <td>-0.020124</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.027379</td>\n",
       "      <td>0.033591</td>\n",
       "      <td>think socially inept feel little intimidate pa...</td>\n",
       "      <td>i don t think i m socially inept but i do feel...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312604</th>\n",
       "      <td>0.036642</td>\n",
       "      <td>0.034220</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>-0.011373</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>0.034240</td>\n",
       "      <td>-0.005113</td>\n",
       "      <td>-0.037219</td>\n",
       "      <td>0.024395</td>\n",
       "      <td>0.015452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>-0.016553</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>-0.059728</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>ive try make feel ugly inside</td>\n",
       "      <td>ive tried being against it and it makes me fee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312605</th>\n",
       "      <td>0.024129</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>-0.019150</td>\n",
       "      <td>0.018971</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>-0.052923</td>\n",
       "      <td>-0.003114</td>\n",
       "      <td>0.054250</td>\n",
       "      <td>-0.041765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>-0.005660</td>\n",
       "      <td>-0.018986</td>\n",
       "      <td>-0.001289</td>\n",
       "      <td>-0.025595</td>\n",
       "      <td>-0.028113</td>\n",
       "      <td>-0.017411</td>\n",
       "      <td>angry sad depress feel like damage goods</td>\n",
       "      <td>i am angry sad depressed and feel like damaged...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312606 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             c_0       c_1       c_2       c_3       c_4       c_5       c_6  \\\n",
       "0       0.005049 -0.004521 -0.033797 -0.011138 -0.024820  0.018492 -0.015220   \n",
       "1      -0.039426 -0.004666 -0.022700  0.006759 -0.026521  0.036558 -0.009668   \n",
       "2       0.027580  0.060547 -0.028398 -0.019655  0.017900 -0.028050 -0.067037   \n",
       "3      -0.002705  0.047667 -0.001580 -0.026343  0.015890  0.024639 -0.091406   \n",
       "4       0.014338 -0.047544  0.025093  0.011402 -0.006114 -0.007457 -0.060139   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "312601 -0.020691 -0.014837 -0.003853 -0.028849 -0.007957  0.065683 -0.099857   \n",
       "312602  0.012106  0.056657 -0.035532 -0.017904 -0.007871  0.036242  0.011781   \n",
       "312603 -0.013450 -0.008782  0.033715  0.015963 -0.009311  0.008958 -0.031408   \n",
       "312604  0.036642  0.034220  0.017764 -0.011373  0.013963  0.034240 -0.005113   \n",
       "312605  0.024129  0.017702  0.001352 -0.019150  0.018971  0.034422 -0.052923   \n",
       "\n",
       "             c_7       c_8       c_9  ...     c_761     c_762     c_763  \\\n",
       "0       0.012329 -0.017906  0.047792  ...  0.021197  0.024708 -0.039280   \n",
       "1      -0.055998  0.037705 -0.007174  ... -0.043236  0.020760  0.006370   \n",
       "2       0.006156 -0.047099  0.023724  ...  0.000482  0.009941 -0.061392   \n",
       "3       0.014512  0.029813  0.026739  ...  0.031983 -0.009349 -0.035115   \n",
       "4      -0.041030 -0.072453  0.019001  ... -0.019103  0.007613 -0.013725   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "312601 -0.012396  0.054042 -0.024364  ... -0.040756  0.004264 -0.043533   \n",
       "312602  0.000313  0.005871  0.013937  ...  0.021886  0.003809  0.042900   \n",
       "312603 -0.024701  0.038739  0.009752  ...  0.049028 -0.053441 -0.020124   \n",
       "312604 -0.037219  0.024395  0.015452  ...  0.022900  0.003701 -0.016553   \n",
       "312605 -0.003114  0.054250 -0.041765  ...  0.007954 -0.005660 -0.018986   \n",
       "\n",
       "           c_764     c_765     c_766     c_767  \\\n",
       "0      -0.020336  0.022013 -0.060829 -0.017347   \n",
       "1       0.007713 -0.046003 -0.000759 -0.012333   \n",
       "2      -0.028997  0.022805  0.026034 -0.006839   \n",
       "3       0.008799  0.015763  0.038264 -0.037314   \n",
       "4      -0.008387  0.000549  0.007308  0.002215   \n",
       "...          ...       ...       ...       ...   \n",
       "312601  0.008517  0.032682  0.011600 -0.009431   \n",
       "312602 -0.027610  0.030307  0.061982 -0.047177   \n",
       "312603  0.018001  0.023213  0.027379  0.033591   \n",
       "312604  0.006632  0.018382 -0.059728  0.008445   \n",
       "312605 -0.001289 -0.025595 -0.028113 -0.017411   \n",
       "\n",
       "                                        text_WO_stopwords  \\\n",
       "0                            feel im torture slowly today   \n",
       "1                                     ill feel really bad   \n",
       "2       feel content whatever news tell know rise chal...   \n",
       "3                        love feel invigorate arrive work   \n",
       "4       feel acceptable use first many polish get rece...   \n",
       "...                                                   ...   \n",
       "312601                     feel terrible course fantastic   \n",
       "312602  give permission feel vulnerable feel anxious p...   \n",
       "312603  think socially inept feel little intimidate pa...   \n",
       "312604                      ive try make feel ugly inside   \n",
       "312605           angry sad depress feel like damage goods   \n",
       "\n",
       "                                                     text  label  \n",
       "0        i feel as if im being tortured very slowly today      4  \n",
       "1                                     ill feel really bad      0  \n",
       "2       i feel content with whatever news we are told ...      1  \n",
       "3        i love feeling invigorated when i arrive at work      1  \n",
       "4       i feel is acceptable i used the first of many ...      1  \n",
       "...                                                   ...    ...  \n",
       "312601  i was feeling terrible but of course she did f...      0  \n",
       "312602  i can give myself permission to feel vulnerabl...      4  \n",
       "312603  i don t think i m socially inept but i do feel...      4  \n",
       "312604  ive tried being against it and it makes me fee...      0  \n",
       "312605  i am angry sad depressed and feel like damaged...      0  \n",
       "\n",
       "[312606 rows x 771 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac699e8-4df1-4c02-b42f-c69a78352ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train= train['text_WO_stopwords']\n",
    "y_train= train['label']\n",
    "\n",
    "X_test= test['text_WO_stopwords']\n",
    "y_test= test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a15cbc-fbc7-4284-a8d5-a5749913af2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>...</th>\n",
       "      <th>c_761</th>\n",
       "      <th>c_762</th>\n",
       "      <th>c_763</th>\n",
       "      <th>c_764</th>\n",
       "      <th>c_765</th>\n",
       "      <th>c_766</th>\n",
       "      <th>c_767</th>\n",
       "      <th>text_WO_stopwords</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [c_0, c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9, c_10, c_11, c_12, c_13, c_14, c_15, c_16, c_17, c_18, c_19, c_20, c_21, c_22, c_23, c_24, c_25, c_26, c_27, c_28, c_29, c_30, c_31, c_32, c_33, c_34, c_35, c_36, c_37, c_38, c_39, c_40, c_41, c_42, c_43, c_44, c_45, c_46, c_47, c_48, c_49, c_50, c_51, c_52, c_53, c_54, c_55, c_56, c_57, c_58, c_59, c_60, c_61, c_62, c_63, c_64, c_65, c_66, c_67, c_68, c_69, c_70, c_71, c_72, c_73, c_74, c_75, c_76, c_77, c_78, c_79, c_80, c_81, c_82, c_83, c_84, c_85, c_86, c_87, c_88, c_89, c_90, c_91, c_92, c_93, c_94, c_95, c_96, c_97, c_98, c_99, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 771 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['text_WO_stopwords'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7f121-da48-4d20-b569-a9a949dc9eab",
   "metadata": {},
   "source": [
    "Looks like there are some rows in our dataset which are null after cleaning data, therefore we need to replace those rows with  '' before tokenizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faa8f4fb-7d36-4a48-bbed-7e84113623ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>c_0</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>...</th>\n",
       "      <th>c_761</th>\n",
       "      <th>c_762</th>\n",
       "      <th>c_763</th>\n",
       "      <th>c_764</th>\n",
       "      <th>c_765</th>\n",
       "      <th>c_766</th>\n",
       "      <th>c_767</th>\n",
       "      <th>text_WO_stopwords</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>342462</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>-0.033797</td>\n",
       "      <td>-0.011138</td>\n",
       "      <td>-0.024820</td>\n",
       "      <td>0.018492</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>-0.017906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.024708</td>\n",
       "      <td>-0.039280</td>\n",
       "      <td>-0.020336</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>-0.060829</td>\n",
       "      <td>-0.017347</td>\n",
       "      <td>feel im torture slowly today</td>\n",
       "      <td>i feel as if im being tortured very slowly today</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87990</td>\n",
       "      <td>-0.039426</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>-0.022700</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>-0.009668</td>\n",
       "      <td>-0.055998</td>\n",
       "      <td>0.037705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043236</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.007713</td>\n",
       "      <td>-0.046003</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>-0.012333</td>\n",
       "      <td>ill feel really bad</td>\n",
       "      <td>ill feel really bad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>381594</td>\n",
       "      <td>0.027580</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>-0.028398</td>\n",
       "      <td>-0.019655</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>-0.028050</td>\n",
       "      <td>-0.067037</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>-0.047099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>-0.061392</td>\n",
       "      <td>-0.028997</td>\n",
       "      <td>0.022805</td>\n",
       "      <td>0.026034</td>\n",
       "      <td>-0.006839</td>\n",
       "      <td>feel content whatever news tell know rise chal...</td>\n",
       "      <td>i feel content with whatever news we are told ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>208217</td>\n",
       "      <td>-0.002705</td>\n",
       "      <td>0.047667</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.026343</td>\n",
       "      <td>0.015890</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>-0.091406</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.029813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031983</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.035115</td>\n",
       "      <td>0.008799</td>\n",
       "      <td>0.015763</td>\n",
       "      <td>0.038264</td>\n",
       "      <td>-0.037314</td>\n",
       "      <td>love feel invigorate arrive work</td>\n",
       "      <td>i love feeling invigorated when i arrive at work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>376976</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>-0.047544</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>-0.060139</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.072453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019103</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>-0.013725</td>\n",
       "      <td>-0.008387</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>feel acceptable use first many polish get rece...</td>\n",
       "      <td>i feel is acceptable i used the first of many ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312601</th>\n",
       "      <td>70499</td>\n",
       "      <td>-0.020691</td>\n",
       "      <td>-0.014837</td>\n",
       "      <td>-0.003853</td>\n",
       "      <td>-0.028849</td>\n",
       "      <td>-0.007957</td>\n",
       "      <td>0.065683</td>\n",
       "      <td>-0.099857</td>\n",
       "      <td>-0.012396</td>\n",
       "      <td>0.054042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040756</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>-0.043533</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.032682</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>feel terrible course fantastic</td>\n",
       "      <td>i was feeling terrible but of course she did f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312602</th>\n",
       "      <td>132865</td>\n",
       "      <td>0.012106</td>\n",
       "      <td>0.056657</td>\n",
       "      <td>-0.035532</td>\n",
       "      <td>-0.017904</td>\n",
       "      <td>-0.007871</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.011781</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021886</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>-0.027610</td>\n",
       "      <td>0.030307</td>\n",
       "      <td>0.061982</td>\n",
       "      <td>-0.047177</td>\n",
       "      <td>give permission feel vulnerable feel anxious p...</td>\n",
       "      <td>i can give myself permission to feel vulnerabl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312603</th>\n",
       "      <td>133735</td>\n",
       "      <td>-0.013450</td>\n",
       "      <td>-0.008782</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.015963</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>-0.031408</td>\n",
       "      <td>-0.024701</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049028</td>\n",
       "      <td>-0.053441</td>\n",
       "      <td>-0.020124</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.027379</td>\n",
       "      <td>0.033591</td>\n",
       "      <td>think socially inept feel little intimidate pa...</td>\n",
       "      <td>i don t think i m socially inept but i do feel...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312604</th>\n",
       "      <td>403684</td>\n",
       "      <td>0.036642</td>\n",
       "      <td>0.034220</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>-0.011373</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>0.034240</td>\n",
       "      <td>-0.005113</td>\n",
       "      <td>-0.037219</td>\n",
       "      <td>0.024395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>-0.016553</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>-0.059728</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>ive try make feel ugly inside</td>\n",
       "      <td>ive tried being against it and it makes me fee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312605</th>\n",
       "      <td>76891</td>\n",
       "      <td>0.024129</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>-0.019150</td>\n",
       "      <td>0.018971</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>-0.052923</td>\n",
       "      <td>-0.003114</td>\n",
       "      <td>0.054250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>-0.005660</td>\n",
       "      <td>-0.018986</td>\n",
       "      <td>-0.001289</td>\n",
       "      <td>-0.025595</td>\n",
       "      <td>-0.028113</td>\n",
       "      <td>-0.017411</td>\n",
       "      <td>angry sad depress feel like damage goods</td>\n",
       "      <td>i am angry sad depressed and feel like damaged...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312606 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       c_0       c_1       c_2       c_3       c_4  \\\n",
       "0           342462  0.005049 -0.004521 -0.033797 -0.011138 -0.024820   \n",
       "1            87990 -0.039426 -0.004666 -0.022700  0.006759 -0.026521   \n",
       "2           381594  0.027580  0.060547 -0.028398 -0.019655  0.017900   \n",
       "3           208217 -0.002705  0.047667 -0.001580 -0.026343  0.015890   \n",
       "4           376976  0.014338 -0.047544  0.025093  0.011402 -0.006114   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "312601       70499 -0.020691 -0.014837 -0.003853 -0.028849 -0.007957   \n",
       "312602      132865  0.012106  0.056657 -0.035532 -0.017904 -0.007871   \n",
       "312603      133735 -0.013450 -0.008782  0.033715  0.015963 -0.009311   \n",
       "312604      403684  0.036642  0.034220  0.017764 -0.011373  0.013963   \n",
       "312605       76891  0.024129  0.017702  0.001352 -0.019150  0.018971   \n",
       "\n",
       "             c_5       c_6       c_7       c_8  ...     c_761     c_762  \\\n",
       "0       0.018492 -0.015220  0.012329 -0.017906  ...  0.021197  0.024708   \n",
       "1       0.036558 -0.009668 -0.055998  0.037705  ... -0.043236  0.020760   \n",
       "2      -0.028050 -0.067037  0.006156 -0.047099  ...  0.000482  0.009941   \n",
       "3       0.024639 -0.091406  0.014512  0.029813  ...  0.031983 -0.009349   \n",
       "4      -0.007457 -0.060139 -0.041030 -0.072453  ... -0.019103  0.007613   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "312601  0.065683 -0.099857 -0.012396  0.054042  ... -0.040756  0.004264   \n",
       "312602  0.036242  0.011781  0.000313  0.005871  ...  0.021886  0.003809   \n",
       "312603  0.008958 -0.031408 -0.024701  0.038739  ...  0.049028 -0.053441   \n",
       "312604  0.034240 -0.005113 -0.037219  0.024395  ...  0.022900  0.003701   \n",
       "312605  0.034422 -0.052923 -0.003114  0.054250  ...  0.007954 -0.005660   \n",
       "\n",
       "           c_763     c_764     c_765     c_766     c_767  \\\n",
       "0      -0.039280 -0.020336  0.022013 -0.060829 -0.017347   \n",
       "1       0.006370  0.007713 -0.046003 -0.000759 -0.012333   \n",
       "2      -0.061392 -0.028997  0.022805  0.026034 -0.006839   \n",
       "3      -0.035115  0.008799  0.015763  0.038264 -0.037314   \n",
       "4      -0.013725 -0.008387  0.000549  0.007308  0.002215   \n",
       "...          ...       ...       ...       ...       ...   \n",
       "312601 -0.043533  0.008517  0.032682  0.011600 -0.009431   \n",
       "312602  0.042900 -0.027610  0.030307  0.061982 -0.047177   \n",
       "312603 -0.020124  0.018001  0.023213  0.027379  0.033591   \n",
       "312604 -0.016553  0.006632  0.018382 -0.059728  0.008445   \n",
       "312605 -0.018986 -0.001289 -0.025595 -0.028113 -0.017411   \n",
       "\n",
       "                                        text_WO_stopwords  \\\n",
       "0                            feel im torture slowly today   \n",
       "1                                     ill feel really bad   \n",
       "2       feel content whatever news tell know rise chal...   \n",
       "3                        love feel invigorate arrive work   \n",
       "4       feel acceptable use first many polish get rece...   \n",
       "...                                                   ...   \n",
       "312601                     feel terrible course fantastic   \n",
       "312602  give permission feel vulnerable feel anxious p...   \n",
       "312603  think socially inept feel little intimidate pa...   \n",
       "312604                      ive try make feel ugly inside   \n",
       "312605           angry sad depress feel like damage goods   \n",
       "\n",
       "                                                     text  label  \n",
       "0        i feel as if im being tortured very slowly today      4  \n",
       "1                                     ill feel really bad      0  \n",
       "2       i feel content with whatever news we are told ...      1  \n",
       "3        i love feeling invigorated when i arrive at work      1  \n",
       "4       i feel is acceptable i used the first of many ...      1  \n",
       "...                                                   ...    ...  \n",
       "312601  i was feeling terrible but of course she did f...      0  \n",
       "312602  i can give myself permission to feel vulnerabl...      4  \n",
       "312603  i don t think i m socially inept but i do feel...      4  \n",
       "312604  ive tried being against it and it makes me fee...      0  \n",
       "312605  i am angry sad depressed and feel like damaged...      0  \n",
       "\n",
       "[312606 rows x 772 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe0027-cf63-48e3-8cdb-52c49b124197",
   "metadata": {},
   "source": [
    "The Tokenizer class in TensorFlow's Keras API is used for converting text documents into tokenized sequences, which can then be used for training machine learning models, particularly neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6228c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximu size of the squence train: 48\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(num_words=500000)\n",
    "tkn.fit_on_texts(X_train)\n",
    "tkn.fit_on_texts(X_test)\n",
    "sq_X_train = tkn.texts_to_sequences(X_train)\n",
    "sq_X_test = tkn.texts_to_sequences(X_test)\n",
    "\n",
    "size = max(len(tokens) for tokens in sq_X_train)\n",
    "X_train_pd = pad_sequences(sq_X_train, padding='post', maxlen=size)\n",
    "X_test_pd = pad_sequences(sq_X_test, padding='post', maxlen=size)\n",
    "\n",
    "print(\"The maximu size of the squence train:\", size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6714d11b-ec84-4fde-8e61-f593750a0893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312606, 48)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeacc7bb-13e8-4304-a643-de18717c033c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 00:01:16.008056: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 48, 1000)          56534000  \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128)               433920    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,977,078\n",
      "Trainable params: 56,976,822\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 00:01:16.281704: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-20 00:01:16.283211: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-20 00:01:16.284319: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "input_shape = np.max(X_train_pd) + 1\n",
    "input_shape\n",
    "\n",
    "# defining a sequential model \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=input_shape, output_dim=1000,input_shape=(size,)), #, input_length=size\n",
    "    GRU(units=128),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=6, activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), metrics=['accuracy'], loss=SparseCategoricalCrossentropy())\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4dc7e8-7e95-40be-8187-e1b3f878ba5f",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23644f-dd1e-4aa8-8d1a-fb0fdd354242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 00:01:49.816350: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-20 00:01:49.818037: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-20 00:01:49.819247: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-20 00:01:50.601945: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-20 00:01:50.603766: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-20 00:01:50.604957: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 275/4885 [>.............................] - ETA: 1:10:26 - loss: 1.6041 - accuracy: 0.3323"
     ]
    }
   ],
   "source": [
    "model_training_history= model.fit(X_train_pd, y_train, epochs=10, batch_size=64, validation_data=(X_test_pd, y_test),callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbfeb5-5a87-49a5-95b4-b1a3e3944827",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_color = '#5fa1bc'\n",
    "plt.style.use('seaborn-dark')\n",
    "plt.rcParams['axes.facecolor'] = background_color\n",
    "\n",
    "# Get the epoch with the highest validation accuracy\n",
    "best_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "plt.scatter(best_epoch - 1, history.history['val_accuracy'][best_epoch - 1], color='green', label=f'Best Epoch: {best_epoch}')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d1835-01d7-47cd-9080-152d6e17d444",
   "metadata": {},
   "source": [
    "# 4- Another Method Keras tokenizer and Tensorflow model with balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e80885-e4fd-44b3-a855-25ebf217a705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "scm",
   "language": "python",
   "name": "scm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
